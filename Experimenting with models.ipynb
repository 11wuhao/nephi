{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Russell Ault\n",
    "\n",
    "# This Notebook Contains code for exploring the functionality of the Pytorch Handwriting Recognition Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is clear to me that I need to write interactive code to load a model that we have trained and test it on a validation set, and have it output accuracy and word error rates. \n",
    "\n",
    "## The present way that the library is written is clearly not easily conducive to this\n",
    "\n",
    "## Here are some comments I have about the code and how to improve it:\n",
    "- In evaluating model accuracy a character by character accuracy is being used, not an edit distance. I need to put character and word error rates into the model. I should include a mean and sd of these parameters.\n",
    "- I think I need to just run the validation code right now to see what it does.\n",
    "- I think that the main python module should be refactored to allow its use in other python modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Main Functionality in Notebook fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import os\n",
    "import utils\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.crnn as crnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--trainroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--valroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "parser.add_argument('--imgH', type=int, default=32, help='the height of the input image to network')\n",
    "parser.add_argument('--imgW', type=int, default=100, help='the width of the input image to network')\n",
    "parser.add_argument('--nh', type=int, default=256, help='size of the lstm hidden state')\n",
    "parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='learning rate for Critic, default=0.00005')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--crnn', default='', help=\"path to crnn (to continue training)\")\n",
    "parser.add_argument('--alphabet', type=str, default='0123456789abcdefghijklmnopqrstuvwxyz')\n",
    "parser.add_argument('--experiment', default=None, help='Where to store samples and models')\n",
    "parser.add_argument('--displayInterval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--n_test_disp', type=int, default=10, help='Number of samples to display when test')\n",
    "parser.add_argument('--valInterval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--saveInterval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--adam', action='store_true', help='Whether to use adam (default is rmsprop)')\n",
    "parser.add_argument('--adadelta', action='store_true', help='Whether to use adadelta (default is rmsprop)')\n",
    "parser.add_argument('--keep_ratio', action='store_true', help='whether to keep ratio for image resize')\n",
    "parser.add_argument('--random_sample', action='store_true', help='whether to sample the dataset with random sampler')\n",
    "opt = parser.parse_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment is None:\n",
    "    experiment = 'expr'\n",
    "os.system('mkdir {0}'.format(experiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainroot = \"/home/ubuntu/russell/nephi/data/lmdb/train\"\n",
    "valroot = \"/home/ubuntu/russell/nephi/data/lmdb/val\"\n",
    "batchSize = 64\n",
    "nh = 256                  # size of the LSTM hidden state\n",
    "imgW = 100\n",
    "imgH = 32\n",
    "ngpu = 1\n",
    "beta1 = 0.5\n",
    "lr = 0.0001\n",
    "workers = 10\n",
    "keep_ratio = True\n",
    "\n",
    "alph_file_dylan = \"/home/ubuntu/dylan/nephi/alphabet.txt\"\n",
    "alph_file_russell = \"/home/ubuntu/russell/nephi/alphabet.txt\"\n",
    "alphabet = '0123456789abcdefghijklmnopqrstuvwxyzB- EÂ¬Ã¼.RSÅ«J/DHA:K¤¿ZLGFNTPCOVWIM<8d>Ä<81><9f>,<93>È³¶'\n",
    "#0123456789abcdefghijklmnopqrstuvwxyzW VCGū¬.HM,ILAZ:BTÿSER<BC>JFāP<9F>NDKOȳ<B6>\n",
    "#<A4><8D>()—̈-<84><93>Q<96>/Y<BE>U<>+  # This is what I got from Dylan's file\n",
    "\n",
    "\n",
    "trained_crnn_russell = \"/home/ubuntu/russell/nephi/expr/netCRNN_3870_100.pth\"\n",
    "trained_crnn_dylan = \"/home/ubuntu/dylan/nephi/expr/netCRNN_3210_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  2749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6b604c7960>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cuda = True\n",
    "\n",
    "#if torch.cuda.is_available() and not cuda:\n",
    "#    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "train_dataset = dataset.lmdbDataset(root=trainroot)\n",
    "sampler = dataset.randomSequentialSampler(train_dataset, batchSize)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchSize, sampler=sampler,\n",
    "    num_workers=int(workers),\n",
    "    collate_fn=dataset.alignCollate(imgH=imgH, imgW=imgW, keep_ratio=keep_ratio))\n",
    "test_dataset = dataset.lmdbDataset(\n",
    "    root=valroot, transform=dataset.resizeNormalize((imgW, imgH)))   # I have changed this line from the original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the two alphabets\n",
    "alphabet_russell = ''\n",
    "alphabet_dylan = ''\n",
    "\n",
    "with open(alph_file_russell, 'r') as myfile:\n",
    "    alphabet_russell = myfile.read()\n",
    "with open(alph_file_dylan, 'r') as myfile:\n",
    "    alphabet_dylan = myfile.read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dylan ALphabet and model first\n",
    "alphabet = alphabet_dylan\n",
    "\n",
    "nclass = len(alphabet) + 1\n",
    "nc = 1\n",
    "\n",
    "converter = utils.strLabelConverter(alphabet)\n",
    "criterion = CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on crnn\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = crnn.CRNN(imgH, nc, nclass, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN (\n",
       "  (cnn): Sequential (\n",
       "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu0): ReLU (inplace)\n",
       "    (pooling0): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU (inplace)\n",
       "    (pooling1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (relu2): ReLU (inplace)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU (inplace)\n",
       "    (pooling2): MaxPool2d (size=(2, 2), stride=(2, 1), dilation=(1, 1))\n",
       "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (relu4): ReLU (inplace)\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu5): ReLU (inplace)\n",
       "    (pooling3): MaxPool2d (size=(2, 2), stride=(2, 1), dilation=(1, 1))\n",
       "    (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (relu6): ReLU (inplace)\n",
       "  )\n",
       "  (rnn): Sequential (\n",
       "    (0): BidirectionalLSTM (\n",
       "      (rnn): LSTM(512, 256, bidirectional=True)\n",
       "      (embedding): Linear (512 -> 256)\n",
       "    )\n",
       "    (1): BidirectionalLSTM (\n",
       "      (rnn): LSTM(256, 256, bidirectional=True)\n",
       "      (embedding): Linear (512 -> 97)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crnn.apply(weights_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the above unexpected key error, I will assume that when I try to run the original code with a validation epoch number, I will get the same error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "text = torch.IntTensor(batchSize * 5)          # RA: I don't understand why the text has this size\n",
    "length = torch.IntTensor(batchSize)\n",
    "\n",
    "if cuda:\n",
    "    crnn.cuda()\n",
    "    crnn = torch.nn.DataParallel(crnn, device_ids=range(ngpu))\n",
    "    image = image.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dylan's pretrained model first\n",
    "trained_crnn = trained_crnn_dylan\n",
    "if trained_crnn != '':\n",
    "    print('loading pretrained model from %s' % trained_crnn)\n",
    "    crnn.load_state_dict(torch.load(trained_crnn))\n",
    "print(crnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Variable(image)\n",
    "text = Variable(text)\n",
    "length = Variable(length)\n",
    "\n",
    "# loss averager\n",
    "loss_avg = utils.averager()\n",
    "\n",
    "# setup optimizer\n",
    "if adam:\n",
    "    optimizer = optim.Adam(crnn.parameters(), lr=lr,\n",
    "                           betas=(beta1, 0.999))\n",
    "elif adadelta:\n",
    "    optimizer = optim.Adadelta(crnn.parameters(), lr=lr)\n",
    "else:\n",
    "    optimizer = optim.RMSprop(crnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is where I will test out the code.\n",
    "\n",
    "### First order of business is to see what val outputs currently on these pretrained models using the test set.\n",
    "### Then add word and character error rate and a way to calculate mean and standard deviation of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(net, dataset, criterion, max_iter=100):\n",
    "    print('Start val')\n",
    "\n",
    "    for p in crnn.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    net.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, shuffle=True, batch_size=batchSize, num_workers=int(workers))\n",
    "    val_iter = iter(data_loader)\n",
    "\n",
    "    i = 0\n",
    "    n_correct = 0\n",
    "    loss_avg = utils.averager()\n",
    "\n",
    "    max_iter = min(max_iter, len(data_loader))\n",
    "    for i in range(max_iter):\n",
    "        data = val_iter.next()\n",
    "        i += 1\n",
    "        cpu_images, cpu_texts = data\n",
    "        batch_size = cpu_images.size(0)\n",
    "        utils.loadData(image, cpu_images)\n",
    "        t, l = converter.encode(cpu_texts)\n",
    "        utils.loadData(text, t)\n",
    "        utils.loadData(length, l)\n",
    "\n",
    "        preds = crnn(image)\n",
    "        preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "        cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "        loss_avg.add(cost)\n",
    "        \n",
    "        \n",
    "        # RA: While I am not sure yet, it looks like a greedy decoder and not beam search is being used here\n",
    "        # Also, a simple character by character accuracy is being used, not an edit distance.\n",
    "        # Case is ignored in the accuracy, which is not ideal for an actual working system\n",
    "        \n",
    "        _, preds = preds.max(2)\n",
    "        preds = preds.squeeze(2)\n",
    "        preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "        for pred, target in zip(sim_preds, cpu_texts):\n",
    "            if pred == target.lower():\n",
    "                n_correct += 1\n",
    "\n",
    "    raw_preds = converter.decode(preds.data, preds_size.data, raw=True)[:n_test_disp]\n",
    "    for raw_pred, pred, gt in zip(raw_preds, sim_preds, cpu_texts):\n",
    "        print('%-20s => %-20s, gt: %-20s' % (raw_pred, pred, gt))\n",
    "\n",
    "    accuracy = n_correct / float(max_iter * batchSize)\n",
    "    print('Test loss: %f, accuray: %f' % (loss_avg.val(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBatch(net, criterion, optimizer):\n",
    "    data = train_iter.next()\n",
    "    cpu_images, cpu_texts = data\n",
    "    batch_size = cpu_images.size(0)\n",
    "    utils.loadData(image, cpu_images)\n",
    "    t, l = converter.encode(cpu_texts)\n",
    "    utils.loadData(text, t)\n",
    "    utils.loadData(length, l)\n",
    "\n",
    "    preds = crnn(image)\n",
    "    preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "    cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "    crnn.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(niter):\n",
    "    train_iter = iter(train_loader)\n",
    "    i = 0\n",
    "    while i < len(train_loader):\n",
    "        for p in crnn.parameters():\n",
    "            p.requires_grad = True\n",
    "        crnn.train()\n",
    "\n",
    "        cost = trainBatch(crnn, criterion, optimizer)\n",
    "        loss_avg.add(cost)\n",
    "        i += 1\n",
    "\n",
    "        if i % displayInterval == 0:\n",
    "            print('[%d/%d][%d/%d] Loss: %f' %\n",
    "                  (epoch, niter, i, len(train_loader), loss_avg.val()))\n",
    "            loss_avg.reset()\n",
    "\n",
    "        if i % valInterval == 0:\n",
    "            val(crnn, test_dataset, criterion)\n",
    "\n",
    "        # do checkpointing\n",
    "        if i % saveInterval == 0:\n",
    "            torch.save(\n",
    "                crnn.state_dict(), '{0}/netCRNN_{1}_{2}.pth'.format(experiment, epoch, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
