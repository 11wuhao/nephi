{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Russell Ault\n",
    "\n",
    "# This Notebook Contains code for exploring the functionality of the Pytorch Handwriting Recognition Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is clear to me that I need to write interactive code to load a model that we have trained and test it on a validation set, and have it output accuracy and word error rates. \n",
    "\n",
    "## The present way that the library is written is clearly not easily conducive to this\n",
    "\n",
    "## Here are some comments I have about the code and how to improve it:\n",
    "- In evaluating model accuracy a character by character accuracy is being used, not an edit distance. I need to put character and word error rates into the model. I should include a mean and sd of these parameters.\n",
    "- I think I need to just run the validation code right now to see what it does.\n",
    "- I think that the main python module should be refactored to allow its use in other python modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Main Functionality in Notebook fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import os\n",
    "import utils\n",
    "import dataset\n",
    "\n",
    "import models.crnn as crnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--trainroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--valroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "parser.add_argument('--imgH', type=int, default=32, help='the height of the input image to network')\n",
    "parser.add_argument('--imgW', type=int, default=100, help='the width of the input image to network')\n",
    "parser.add_argument('--nh', type=int, default=256, help='size of the lstm hidden state')\n",
    "parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='learning rate for Critic, default=0.00005')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--crnn', default='', help=\"path to crnn (to continue training)\")\n",
    "parser.add_argument('--alphabet', type=str, default='0123456789abcdefghijklmnopqrstuvwxyz')\n",
    "parser.add_argument('--experiment', default=None, help='Where to store samples and models')\n",
    "parser.add_argument('--displayInterval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--n_test_disp', type=int, default=10, help='Number of samples to display when test')\n",
    "parser.add_argument('--valInterval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--saveInterval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--adam', action='store_true', help='Whether to use adam (default is rmsprop)')\n",
    "parser.add_argument('--adadelta', action='store_true', help='Whether to use adadelta (default is rmsprop)')\n",
    "parser.add_argument('--keep_ratio', action='store_true', help='whether to keep ratio for image resize')\n",
    "parser.add_argument('--random_sample', action='store_true', help='whether to sample the dataset with random sampler')\n",
    "opt = parser.parse_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment is None:\n",
    "    experiment = 'expr'\n",
    "os.system('mkdir {0}'.format(experiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainroot = \"/home/ubuntu/russell/nephi/data/lmdb/train\"\n",
    "valroot = \"/home/ubuntu/russell/nephi/data/lmdb/val\"\n",
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cuda = True\n",
    "\n",
    "if torch.cuda.is_available() and not cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "train_dataset = dataset.lmdbDataset(root=trainroot)\n",
    "assert train_dataset\n",
    "if not random_sample:\n",
    "    sampler = dataset.randomSequentialSampler(train_dataset, batchSize)\n",
    "else:\n",
    "    sampler = None\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchSize, sampler=sampler,\n",
    "    num_workers=int(workers),\n",
    "    collate_fn=dataset.alignCollate(imgH=imgH, imgW=imgW, keep_ratio=keep_ratio))\n",
    "test_dataset = dataset.lmdbDataset(\n",
    "    root=valroot, transform=dataset.resizeNormalize((100, 32)))\n",
    "\n",
    "if os.path.exists('alphabet.txt'):\n",
    "    alphabet = ''\n",
    "    with open('alphabet.txt', 'r') as myfile:\n",
    "        alphabet = myfile.read()\n",
    "\n",
    "    if len(alphabet)>1:\n",
    "        alphabet = alphabet\n",
    "\n",
    "\n",
    "nclass = len(alphabet) + 1\n",
    "nc = 1\n",
    "\n",
    "converter = utils.strLabelConverter(alphabet)\n",
    "criterion = CTCLoss()\n",
    "\n",
    "\n",
    "# custom weights initialization called on crnn\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "crnn = crnn.CRNN(imgH, nc, nclass, nh)\n",
    "crnn.apply(weights_init)\n",
    "if crnn != '':\n",
    "    print('loading pretrained model from %s' % crnn)\n",
    "    crnn.load_state_dict(torch.load(crnn))\n",
    "print(crnn)\n",
    "\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "text = torch.IntTensor(batchSize * 5)          # RA: I don't understand why the text has this size\n",
    "length = torch.IntTensor(batchSize)\n",
    "\n",
    "if cuda:\n",
    "    crnn.cuda()\n",
    "    crnn = torch.nn.DataParallel(crnn, device_ids=range(ngpu))\n",
    "    image = image.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "image = Variable(image)\n",
    "text = Variable(text)\n",
    "length = Variable(length)\n",
    "\n",
    "# loss averager\n",
    "loss_avg = utils.averager()\n",
    "\n",
    "# setup optimizer\n",
    "if adam:\n",
    "    optimizer = optim.Adam(crnn.parameters(), lr=lr,\n",
    "                           betas=(beta1, 0.999))\n",
    "elif adadelta:\n",
    "    optimizer = optim.Adadelta(crnn.parameters(), lr=lr)\n",
    "else:\n",
    "    optimizer = optim.RMSprop(crnn.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def val(net, dataset, criterion, max_iter=100):\n",
    "    print('Start val')\n",
    "\n",
    "    for p in crnn.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    net.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, shuffle=True, batch_size=batchSize, num_workers=int(workers))\n",
    "    val_iter = iter(data_loader)\n",
    "\n",
    "    i = 0\n",
    "    n_correct = 0\n",
    "    loss_avg = utils.averager()\n",
    "\n",
    "    max_iter = min(max_iter, len(data_loader))\n",
    "    for i in range(max_iter):\n",
    "        data = val_iter.next()\n",
    "        i += 1\n",
    "        cpu_images, cpu_texts = data\n",
    "        batch_size = cpu_images.size(0)\n",
    "        utils.loadData(image, cpu_images)\n",
    "        t, l = converter.encode(cpu_texts)\n",
    "        utils.loadData(text, t)\n",
    "        utils.loadData(length, l)\n",
    "\n",
    "        preds = crnn(image)\n",
    "        preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "        cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "        loss_avg.add(cost)\n",
    "        \n",
    "        \n",
    "        # RA: While I am not sure yet, it looks like a greedy decoder and not beam search is being used here\n",
    "        # Also, a simple character by character accuracy is being used, not an edit distance.\n",
    "        # Case is ignored in the accuracy, which is not ideal for an actual working system\n",
    "        \n",
    "        _, preds = preds.max(2)\n",
    "        preds = preds.squeeze(2)\n",
    "        preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "        for pred, target in zip(sim_preds, cpu_texts):\n",
    "            if pred == target.lower():\n",
    "                n_correct += 1\n",
    "\n",
    "    raw_preds = converter.decode(preds.data, preds_size.data, raw=True)[:n_test_disp]\n",
    "    for raw_pred, pred, gt in zip(raw_preds, sim_preds, cpu_texts):\n",
    "        print('%-20s => %-20s, gt: %-20s' % (raw_pred, pred, gt))\n",
    "\n",
    "    accuracy = n_correct / float(max_iter * batchSize)\n",
    "    print('Test loss: %f, accuray: %f' % (loss_avg.val(), accuracy))\n",
    "\n",
    "\n",
    "def trainBatch(net, criterion, optimizer):\n",
    "    data = train_iter.next()\n",
    "    cpu_images, cpu_texts = data\n",
    "    batch_size = cpu_images.size(0)\n",
    "    utils.loadData(image, cpu_images)\n",
    "    t, l = converter.encode(cpu_texts)\n",
    "    utils.loadData(text, t)\n",
    "    utils.loadData(length, l)\n",
    "\n",
    "    preds = crnn(image)\n",
    "    preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "    cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "    crnn.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    return cost\n",
    "\n",
    "\n",
    "for epoch in range(niter):\n",
    "    train_iter = iter(train_loader)\n",
    "    i = 0\n",
    "    while i < len(train_loader):\n",
    "        for p in crnn.parameters():\n",
    "            p.requires_grad = True\n",
    "        crnn.train()\n",
    "\n",
    "        cost = trainBatch(crnn, criterion, optimizer)\n",
    "        loss_avg.add(cost)\n",
    "        i += 1\n",
    "\n",
    "        if i % displayInterval == 0:\n",
    "            print('[%d/%d][%d/%d] Loss: %f' %\n",
    "                  (epoch, niter, i, len(train_loader), loss_avg.val()))\n",
    "            loss_avg.reset()\n",
    "\n",
    "        if i % valInterval == 0:\n",
    "            val(crnn, test_dataset, criterion)\n",
    "\n",
    "        # do checkpointing\n",
    "        if i % saveInterval == 0:\n",
    "            torch.save(\n",
    "                crnn.state_dict(), '{0}/netCRNN_{1}_{2}.pth'.format(experiment, epoch, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
