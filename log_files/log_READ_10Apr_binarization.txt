Running with options: Namespace(adadelta=False, adam=False, alphabet='0123456789abcdefghijklmnopqrstuvwxyz', batchSize=2, beta1=0.5, crnn='', cuda=True, displayInterval=120, experiment='experiments/expr_READ_10Apr_binarize', imgH=60, imgW=240, keep_ratio=True, lr=0.0001, n_test_disp=10, ngpu=1, nh=256, niter=200, random_sample=False, saveEpoch=5, test_file='test_file', test_icfhr=False, trainroot='/deep_data/nephi/data/lmdb_read_bin/train', valEpoch=5, valroot='/deep_data/nephi/data/lmdb_read_bin/val', workers=10)
Random Seed:  5547
This is the alphabet:
40FreihatnūfsRb¬:vdgüomw.DlENBcLkPzAOuVȳGSCIH19Tj62,W5KōZpäÿMJ38x()āÖqß̄—öU7̈ē-Q<>/¾yY+ 
Your neural network: DataParallel(
  (module): CRNN(
    (cnn): Sequential(
      (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu0): ReLU(inplace)
      (pooling0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1): ReLU(inplace)
      (pooling1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (relu2): ReLU(inplace)
      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3): ReLU(inplace)
      (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu4): ReLU(inplace)
      (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5): ReLU(inplace)
      (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))
      (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu6): ReLU(inplace)
    )
    (rnn): Sequential(
      (0): BidirectionalLSTM(
        (rnn): LSTM(512, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): BidirectionalLSTM(
        (rnn): LSTM(256, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=89, bias=True)
      )
    )
  )
)
Starting training...
[0/200][120/4184] Loss: 90.737152
[0/200][240/4184] Loss: 72.964146
[0/200][360/4184] Loss: 75.532799
[0/200][480/4184] Loss: 73.500466
[0/200][600/4184] Loss: 69.930226
[0/200][720/4184] Loss: 72.094525
[0/200][840/4184] Loss: 63.818462
[0/200][960/4184] Loss: 61.847489
[0/200][1080/4184] Loss: 57.598921
[0/200][1200/4184] Loss: 54.283086
[0/200][1320/4184] Loss: 48.487959
[0/200][1440/4184] Loss: 47.037653
[0/200][1560/4184] Loss: 44.353754
[0/200][1680/4184] Loss: 40.129444
[0/200][1800/4184] Loss: 38.761615
[0/200][1920/4184] Loss: 34.573955
[0/200][2040/4184] Loss: 30.873860
[0/200][2160/4184] Loss: 31.798479
[0/200][2280/4184] Loss: 26.906864
[0/200][2400/4184] Loss: 26.452160
[0/200][2520/4184] Loss: 25.947538
[0/200][2640/4184] Loss: 25.866155
[0/200][2760/4184] Loss: 23.394964
[0/200][2880/4184] Loss: 21.107986
[0/200][3000/4184] Loss: 22.117780
[0/200][3120/4184] Loss: 22.512229
[0/200][3240/4184] Loss: 20.397280
[0/200][3360/4184] Loss: 20.396379
[0/200][3480/4184] Loss: 19.134900
[0/200][3600/4184] Loss: 19.018435
[0/200][3720/4184] Loss: 17.860528
[0/200][3840/4184] Loss: 19.429754
[0/200][3960/4184] Loss: 18.828874
[0/200][4080/4184] Loss: 16.581302
Start validation set
h                    => h                   , gt: hanns Stūben Vol.   
Total number of images in validation set:     1043
Test loss: 23.380409, accuracy: 0.003831
Character error rate mean: 0.6557; Character error rate sd: 1.0202
Word error rate mean: 1.3791; Word error rate sd: 0.9061
Start validation set
-----------------------------------------------nn--dd---   d-------i-s---e-----  ----------t----e----l--------------i-----tt--------l---e-----d------i--g------------AA-----ll---ll---s---------------------------------------- => nd dise telitledigAlls, gt: vnd dise Stel nit ledig, Alls
-----------------------------tt-----s----------- --d------------a----------b----------e-------n----------h--------------------------------e----------rr---------- -b----------l----e----------------bb--------e-----------n---- => ts dabenher bleben  , gt: hats dabej Zūūerbleiben. 
Total number of images in validation set:     2000
Test loss: nan, accuracy: 0.008500
Character error rate mean: 0.4239; Character error rate sd: 0.8822
Word error rate mean: 1.0459; Word error rate sd: 0.6714
Saving epoch experiments/expr_READ_10Apr_binarize/netCRNN_0_4184.pth
[1/200][120/4184] Loss: 17.259439
[1/200][240/4184] Loss: 15.531540
[1/200][360/4184] Loss: 19.417009
[1/200][480/4184] Loss: 17.883358
[1/200][600/4184] Loss: 15.456746
[1/200][720/4184] Loss: 14.931995
[1/200][840/4184] Loss: 15.830056
[1/200][960/4184] Loss: 14.845288
[1/200][1080/4184] Loss: 15.053885
[1/200][1200/4184] Loss: 15.577269
[1/200][1320/4184] Loss: 16.285223
[1/200][1440/4184] Loss: 15.473149
[1/200][1560/4184] Loss: 13.368657
[1/200][1680/4184] Loss: 12.704070
[1/200][1800/4184] Loss: 12.891965
[1/200][1920/4184] Loss: 13.753152
[1/200][2040/4184] Loss: 13.958283
[1/200][2160/4184] Loss: 14.178972
[1/200][2280/4184] Loss: 13.705709
[1/200][2400/4184] Loss: 13.520130
[1/200][2520/4184] Loss: 12.904120
[1/200][2640/4184] Loss: 11.323105
[1/200][2760/4184] Loss: 12.186446
[1/200][2880/4184] Loss: 11.539304
[1/200][3000/4184] Loss: 12.986759
[1/200][3120/4184] Loss: 13.522947
[1/200][3240/4184] Loss: 11.667291
[1/200][3360/4184] Loss: 11.949326
[1/200][3480/4184] Loss: 13.597790
[1/200][3600/4184] Loss: 11.034534
[1/200][3720/4184] Loss: 11.215902
[1/200][3840/4184] Loss: 10.856825
[1/200][3960/4184] Loss: 11.456212
[1/200][4080/4184] Loss: 11.747714
[2/200][120/4184] Loss: 10.400512
[2/200][240/4184] Loss: 10.949950
[2/200][360/4184] Loss: 10.933291
[2/200][480/4184] Loss: 10.160053
[2/200][600/4184] Loss: 12.066826
[2/200][720/4184] Loss: 11.799657
[2/200][840/4184] Loss: 9.253182
[2/200][960/4184] Loss: 11.421473
[2/200][1080/4184] Loss: 9.562799
[2/200][1200/4184] Loss: 9.433811
[2/200][1320/4184] Loss: 11.198557
[2/200][1440/4184] Loss: 9.300697
[2/200][1560/4184] Loss: 9.350580
[2/200][1680/4184] Loss: 9.452040
[2/200][1800/4184] Loss: 10.813066
[2/200][1920/4184] Loss: 9.508579
[2/200][2040/4184] Loss: 9.057147
[2/200][2160/4184] Loss: 9.453002
[2/200][2280/4184] Loss: 8.421862
[2/200][2400/4184] Loss: 9.242763
[2/200][2520/4184] Loss: 8.286241
[2/200][2640/4184] Loss: 8.988115
[2/200][2760/4184] Loss: 8.664128
[2/200][2880/4184] Loss: 10.315127
[2/200][3000/4184] Loss: 9.586639
[2/200][3120/4184] Loss: 10.322172
[2/200][3240/4184] Loss: 10.662469
[2/200][3360/4184] Loss: 8.511527
[2/200][3480/4184] Loss: 8.010080
[2/200][3600/4184] Loss: 8.796590
[2/200][3720/4184] Loss: 8.594705
[2/200][3840/4184] Loss: 7.277514
[2/200][3960/4184] Loss: 8.254793
[2/200][4080/4184] Loss: 8.454670
[3/200][120/4184] Loss: 8.256366
[3/200][240/4184] Loss: 9.478411
[3/200][360/4184] Loss: 7.765079
[3/200][480/4184] Loss: 7.953737
[3/200][600/4184] Loss: 7.156867
[3/200][720/4184] Loss: 10.147082
[3/200][840/4184] Loss: 8.010403
[3/200][960/4184] Loss: 7.812075
[3/200][1080/4184] Loss: 8.685009
[3/200][1200/4184] Loss: 7.807360
[3/200][1320/4184] Loss: 8.966372
[3/200][1440/4184] Loss: 7.684702
[3/200][1560/4184] Loss: 7.708029
[3/200][1680/4184] Loss: 8.602708
[3/200][1800/4184] Loss: 8.105317
[3/200][1920/4184] Loss: 7.783859
[3/200][2040/4184] Loss: 7.006879
[3/200][2160/4184] Loss: 7.762402
[3/200][2280/4184] Loss: 7.340672
[3/200][2400/4184] Loss: 8.212677
[3/200][2520/4184] Loss: 7.140850
[3/200][2640/4184] Loss: 7.362612
[3/200][2760/4184] Loss: 7.917890
[3/200][2880/4184] Loss: 7.450142
[3/200][3000/4184] Loss: 7.531167
[3/200][3120/4184] Loss: 7.441568
[3/200][3240/4184] Loss: 7.210341
[3/200][3360/4184] Loss: 6.885158
[3/200][3480/4184] Loss: 7.171475
[3/200][3600/4184] Loss: 8.035910
[3/200][3720/4184] Loss: 6.743577
[3/200][3840/4184] Loss: 6.967180
[3/200][3960/4184] Loss: 8.138730
[3/200][4080/4184] Loss: 6.530417
[4/200][120/4184] Loss: 7.049665
[4/200][240/4184] Loss: 7.484611
[4/200][360/4184] Loss: 7.149828
[4/200][480/4184] Loss: nan
[4/200][600/4184] Loss: 6.499846
[4/200][720/4184] Loss: 7.052892
[4/200][840/4184] Loss: 7.177834
[4/200][960/4184] Loss: 5.927503
[4/200][1080/4184] Loss: 6.824530
[4/200][1200/4184] Loss: 6.503927
[4/200][1320/4184] Loss: 6.070611
[4/200][1440/4184] Loss: 6.745185
[4/200][1560/4184] Loss: 5.681533
[4/200][1680/4184] Loss: 7.760642
[4/200][1800/4184] Loss: 7.147892
[4/200][1920/4184] Loss: 6.021494
[4/200][2040/4184] Loss: 6.118744
[4/200][2160/4184] Loss: 6.341089
[4/200][2280/4184] Loss: 5.996068
[4/200][2400/4184] Loss: 6.351472
[4/200][2520/4184] Loss: 5.361966
[4/200][2640/4184] Loss: 5.562826
[4/200][2760/4184] Loss: 5.742134
[4/200][2880/4184] Loss: 6.755132
[4/200][3000/4184] Loss: 5.483901
[4/200][3120/4184] Loss: 5.116321
[4/200][3240/4184] Loss: 7.379872
[4/200][3360/4184] Loss: 6.198936
[4/200][3480/4184] Loss: 6.014300
[4/200][3600/4184] Loss: 6.835150
[4/200][3720/4184] Loss: 5.207119
[4/200][3840/4184] Loss: 6.103020
[4/200][3960/4184] Loss: 5.938965
[4/200][4080/4184] Loss: 6.033152
[5/200][120/4184] Loss: 6.346211
[5/200][240/4184] Loss: 5.390212
[5/200][360/4184] Loss: 5.189008
[5/200][480/4184] Loss: 6.162150
[5/200][600/4184] Loss: 7.897054
[5/200][720/4184] Loss: 6.665149
[5/200][840/4184] Loss: 5.372453
[5/200][960/4184] Loss: 5.031277
[5/200][1080/4184] Loss: 7.014541
[5/200][1200/4184] Loss: 5.240541
[5/200][1320/4184] Loss: 6.454656
[5/200][1440/4184] Loss: 5.945626
[5/200][1560/4184] Loss: 4.858125
[5/200][1680/4184] Loss: 5.663027
[5/200][1800/4184] Loss: 5.831664
[5/200][1920/4184] Loss: 6.312422
[5/200][2040/4184] Loss: 4.908486
[5/200][2160/4184] Loss: 5.571694
[5/200][2280/4184] Loss: 4.595433
[5/200][2400/4184] Loss: 5.251556
[5/200][2520/4184] Loss: 6.578001
[5/200][2640/4184] Loss: 6.683154
[5/200][2760/4184] Loss: 5.909088
[5/200][2880/4184] Loss: 5.089812
[5/200][3000/4184] Loss: 5.984932
[5/200][3120/4184] Loss: 5.445902
[5/200][3240/4184] Loss: 6.927214
[5/200][3360/4184] Loss: 6.773349
[5/200][3480/4184] Loss: 5.044896
[5/200][3600/4184] Loss: 5.326147
[5/200][3720/4184] Loss: 5.219418
[5/200][3840/4184] Loss: 4.669550
[5/200][3960/4184] Loss: 5.043344
[5/200][4080/4184] Loss: 4.270808
Start validation set
D                    => D                   , gt: Der wiert am Creūz. 
Total number of images in validation set:     1043
Test loss: 8.800943, accuracy: 0.166667
Character error rate mean: 0.2193; Character error rate sd: 0.6779
Word error rate mean: 0.5259; Word error rate sd: 0.4296
Start validation set
--P-----------rr-----------ūū----g---------g--------ee----------n--- => Prūggen             , gt: Prūggen.            
------------------------------s------------------------------------- => s                   , gt: 67                  
Total number of images in validation set:     2000
Test loss: 5.553113, accuracy: 0.286500
Character error rate mean: 0.1223; Character error rate sd: 0.3576
Word error rate mean: 0.3684; Word error rate sd: 0.3895
Saving epoch experiments/expr_READ_10Apr_binarize/netCRNN_5_4184.pth
[6/200][120/4184] Loss: 4.691853
[6/200][240/4184] Loss: 4.875703
[6/200][360/4184] Loss: 5.187156
[6/200][480/4184] Loss: 7.155143
[6/200][600/4184] Loss: 5.290243
[6/200][720/4184] Loss: 4.836517
[6/200][840/4184] Loss: 5.825528
[6/200][960/4184] Loss: 4.607286
[6/200][1080/4184] Loss: 5.802377
[6/200][1200/4184] Loss: 5.102884
[6/200][1320/4184] Loss: 4.845686
[6/200][1440/4184] Loss: 4.604193
[6/200][1560/4184] Loss: 5.036587
[6/200][1680/4184] Loss: 4.281670
[6/200][1800/4184] Loss: 4.358300
[6/200][1920/4184] Loss: 4.540020
[6/200][2040/4184] Loss: 4.045673
[6/200][2160/4184] Loss: 4.835869
[6/200][2280/4184] Loss: 4.925761
[6/200][2400/4184] Loss: 6.593426
[6/200][2520/4184] Loss: 4.975116
[6/200][2640/4184] Loss: 5.252726
[6/200][2760/4184] Loss: 4.503077
[6/200][2880/4184] Loss: 4.727009
[6/200][3000/4184] Loss: 4.001636
[6/200][3120/4184] Loss: 4.168346
[6/200][3240/4184] Loss: 5.152624
[6/200][3360/4184] Loss: 5.231124
[6/200][3480/4184] Loss: 4.428580
[6/200][3600/4184] Loss: 5.315476
[6/200][3720/4184] Loss: 3.922722
[6/200][3840/4184] Loss: 5.741408
[6/200][3960/4184] Loss: 4.177758
[6/200][4080/4184] Loss: 4.471830
[7/200][120/4184] Loss: 4.271945
[7/200][240/4184] Loss: 5.327259
[7/200][360/4184] Loss: 5.028279
[7/200][480/4184] Loss: 4.208333
[7/200][600/4184] Loss: 3.795067
[7/200][720/4184] Loss: 5.310153
[7/200][840/4184] Loss: 4.549073
[7/200][960/4184] Loss: 4.347803
[7/200][1080/4184] Loss: 4.408759
[7/200][1200/4184] Loss: 4.096722
[7/200][1320/4184] Loss: 4.396453
[7/200][1440/4184] Loss: 3.862097
[7/200][1560/4184] Loss: nan
[7/200][1680/4184] Loss: 4.482347
[7/200][1800/4184] Loss: 3.684216
[7/200][1920/4184] Loss: 4.488257
[7/200][2040/4184] Loss: 6.309644
[7/200][2160/4184] Loss: 3.313730
[7/200][2280/4184] Loss: 4.078110
[7/200][2400/4184] Loss: 3.777999
[7/200][2520/4184] Loss: 4.391202
[7/200][2640/4184] Loss: 3.781573
[7/200][2760/4184] Loss: 3.632316
[7/200][2880/4184] Loss: 3.936061
[7/200][3000/4184] Loss: 3.707903
[7/200][3120/4184] Loss: 3.585352
[7/200][3240/4184] Loss: 3.410911
[7/200][3360/4184] Loss: 3.415903
[7/200][3480/4184] Loss: 4.946381
[7/200][3600/4184] Loss: 3.238263
[7/200][3720/4184] Loss: 3.616659
[7/200][3840/4184] Loss: 3.725182
[7/200][3960/4184] Loss: 3.838450
[7/200][4080/4184] Loss: 4.560318
[8/200][120/4184] Loss: 3.929066
[8/200][240/4184] Loss: 3.731948
[8/200][360/4184] Loss: 3.130297
[8/200][480/4184] Loss: 3.363860
[8/200][600/4184] Loss: 4.230389
[8/200][720/4184] Loss: 4.092091
[8/200][840/4184] Loss: 4.122116
[8/200][960/4184] Loss: 5.287896
[8/200][1080/4184] Loss: 3.571209
[8/200][1200/4184] Loss: 3.743467
[8/200][1320/4184] Loss: 4.330688
[8/200][1440/4184] Loss: 3.672352
[8/200][1560/4184] Loss: 3.264144
[8/200][1680/4184] Loss: 3.494052
[8/200][1800/4184] Loss: 3.534772
[8/200][1920/4184] Loss: 4.085464
[8/200][2040/4184] Loss: 3.426102
[8/200][2160/4184] Loss: 5.168446
[8/200][2280/4184] Loss: 3.223400
[8/200][2400/4184] Loss: 3.590032
[8/200][2520/4184] Loss: 3.629454
[8/200][2640/4184] Loss: 3.679068
[8/200][2760/4184] Loss: 3.535616
[8/200][2880/4184] Loss: 3.814470
[8/200][3000/4184] Loss: 3.243060
[8/200][3120/4184] Loss: 3.390200
[8/200][3240/4184] Loss: 4.494483
[8/200][3360/4184] Loss: 3.258766
[8/200][3480/4184] Loss: 4.591329
[8/200][3600/4184] Loss: 3.412246
[8/200][3720/4184] Loss: 3.365673
[8/200][3840/4184] Loss: 3.893343
[8/200][3960/4184] Loss: 3.164717
[8/200][4080/4184] Loss: 3.305744
[9/200][120/4184] Loss: 3.331319
[9/200][240/4184] Loss: 3.455425
[9/200][360/4184] Loss: 2.883581
[9/200][480/4184] Loss: 4.953864
[9/200][600/4184] Loss: 3.176700
[9/200][720/4184] Loss: 4.646474
[9/200][840/4184] Loss: 3.334423
[9/200][960/4184] Loss: 3.319070
[9/200][1080/4184] Loss: 3.997902
[9/200][1200/4184] Loss: 3.175145
[9/200][1320/4184] Loss: 3.761206
[9/200][1440/4184] Loss: 3.003086
[9/200][1560/4184] Loss: 3.918410
[9/200][1680/4184] Loss: 3.542859
[9/200][1800/4184] Loss: 3.625552
[9/200][1920/4184] Loss: 3.298453
[9/200][2040/4184] Loss: 2.681967
[9/200][2160/4184] Loss: 3.835962
[9/200][2280/4184] Loss: 3.916331
[9/200][2400/4184] Loss: 3.024607
[9/200][2520/4184] Loss: 3.224676
[9/200][2640/4184] Loss: 3.562481
[9/200][2760/4184] Loss: 2.991333
[9/200][2880/4184] Loss: 3.247513
[9/200][3000/4184] Loss: 2.928048
[9/200][3120/4184] Loss: 2.777340
[9/200][3240/4184] Loss: 3.354898
[9/200][3360/4184] Loss: 3.431273
[9/200][3480/4184] Loss: 3.230767
[9/200][3600/4184] Loss: 2.793660
[9/200][3720/4184] Loss: 2.989455
[9/200][3840/4184] Loss: 3.838834
[9/200][3960/4184] Loss: 2.994550
[9/200][4080/4184] Loss: 2.855088
[10/200][120/4184] Loss: 2.728232
[10/200][240/4184] Loss: 3.176462
[10/200][360/4184] Loss: 2.885875
[10/200][480/4184] Loss: 2.956586
[10/200][600/4184] Loss: 2.601110
[10/200][720/4184] Loss: 3.281589
[10/200][840/4184] Loss: 3.556113
[10/200][960/4184] Loss: 3.111274
[10/200][1080/4184] Loss: 3.701076
[10/200][1200/4184] Loss: 3.145603
[10/200][1320/4184] Loss: 3.238327
[10/200][1440/4184] Loss: 3.694442
[10/200][1560/4184] Loss: 3.597245
[10/200][1680/4184] Loss: 3.208910
[10/200][1800/4184] Loss: 3.231648
[10/200][1920/4184] Loss: 2.678207
[10/200][2040/4184] Loss: 2.705946
[10/200][2160/4184] Loss: 2.442198
[10/200][2280/4184] Loss: 2.556088
[10/200][2400/4184] Loss: 3.330933
[10/200][2520/4184] Loss: 3.744181
[10/200][2640/4184] Loss: 2.844636
[10/200][2760/4184] Loss: 2.801796
[10/200][2880/4184] Loss: 2.782294
[10/200][3000/4184] Loss: 3.454465
[10/200][3120/4184] Loss: 2.900856
[10/200][3240/4184] Loss: 2.713895
[10/200][3360/4184] Loss: 3.046544
[10/200][3480/4184] Loss: 2.350375
[10/200][3600/4184] Loss: 3.034222
[10/200][3720/4184] Loss: 2.711580
[10/200][3840/4184] Loss: 2.449380
[10/200][3960/4184] Loss: 3.329285
[10/200][4080/4184] Loss: 2.750044
Start validation set
H                    => H                   , gt: Hanns Gamber Weber¬ 
Total number of images in validation set:     1043
Test loss: 7.836399, accuracy: 0.205939
Character error rate mean: 0.1579; Character error rate sd: 0.6092
Word error rate mean: 0.4680; Word error rate sd: 0.3914
Start validation set
--------------------A----------m--b----rr---o---s-----ȳȳ--------------------- => Ambrosȳ             , gt: Ambrosȳ             
-G-----------------rr--------o----------tt---------tt-------e-------r-----... => Grotter.            , gt: Grotter.            
Total number of images in validation set:     2000
Test loss: 3.439983, accuracy: 0.443500
Character error rate mean: 0.0602; Character error rate sd: 0.1727
Word error rate mean: 0.2310; Word error rate sd: 0.3103
Saving epoch experiments/expr_READ_10Apr_binarize/netCRNN_10_4184.pth
[11/200][120/4184] Loss: 3.248307
[11/200][240/4184] Loss: 3.054255
[11/200][360/4184] Loss: 4.627109
[11/200][480/4184] Loss: 3.512805
[11/200][600/4184] Loss: 2.735781
[11/200][720/4184] Loss: 3.884616
[11/200][840/4184] Loss: 1.917816
[11/200][960/4184] Loss: 3.030003
[11/200][1080/4184] Loss: 2.769398
[11/200][1200/4184] Loss: 2.406447
[11/200][1320/4184] Loss: 2.526375
[11/200][1440/4184] Loss: 2.931583
[11/200][1560/4184] Loss: 3.102629
[11/200][1680/4184] Loss: 2.703268
[11/200][1800/4184] Loss: 2.899888
[11/200][1920/4184] Loss: 2.290265
[11/200][2040/4184] Loss: 2.577317
[11/200][2160/4184] Loss: 2.650082
[11/200][2280/4184] Loss: 4.042627
[11/200][2400/4184] Loss: 3.883247
[11/200][2520/4184] Loss: 2.382268
[11/200][2640/4184] Loss: 2.591252
[11/200][2760/4184] Loss: 2.774107
[11/200][2880/4184] Loss: 3.683264
[11/200][3000/4184] Loss: 1.804647
[11/200][3120/4184] Loss: 3.297850
[11/200][3240/4184] Loss: 3.112157
[11/200][3360/4184] Loss: 2.505569
[11/200][3480/4184] Loss: 2.080899
[11/200][3600/4184] Loss: 2.419637
[11/200][3720/4184] Loss: 2.622716
[11/200][3840/4184] Loss: 2.500096
[11/200][3960/4184] Loss: 3.946729
[11/200][4080/4184] Loss: 2.641781
[12/200][120/4184] Loss: 2.412391
[12/200][240/4184] Loss: 2.468562
[12/200][360/4184] Loss: 2.522501
[12/200][480/4184] Loss: 3.582220
[12/200][600/4184] Loss: 2.241995
[12/200][720/4184] Loss: 2.113876
[12/200][840/4184] Loss: 2.023476
[12/200][960/4184] Loss: 2.585149
[12/200][1080/4184] Loss: 2.505375
[12/200][1200/4184] Loss: 2.289403
[12/200][1320/4184] Loss: 2.191675
[12/200][1440/4184] Loss: 2.364522
[12/200][1560/4184] Loss: 2.264101
[12/200][1680/4184] Loss: 2.277243
[12/200][1800/4184] Loss: 1.709796
[12/200][1920/4184] Loss: 3.572639
[12/200][2040/4184] Loss: 2.404840
[12/200][2160/4184] Loss: 2.498002
[12/200][2280/4184] Loss: 2.278987
[12/200][2400/4184] Loss: 4.461504
[12/200][2520/4184] Loss: 2.119046
[12/200][2640/4184] Loss: 2.253849
[12/200][2760/4184] Loss: 2.132296
[12/200][2880/4184] Loss: 2.184586
[12/200][3000/4184] Loss: 1.963331
[12/200][3120/4184] Loss: 2.207579
[12/200][3240/4184] Loss: 2.045580
[12/200][3360/4184] Loss: 2.277272
[12/200][3480/4184] Loss: 1.942099
[12/200][3600/4184] Loss: 3.113407
[12/200][3720/4184] Loss: 2.401621
[12/200][3840/4184] Loss: 1.732049
[12/200][3960/4184] Loss: 2.135431
[12/200][4080/4184] Loss: 2.307020
[13/200][120/4184] Loss: 2.019169
[13/200][240/4184] Loss: 2.380294
[13/200][360/4184] Loss: 2.390010
[13/200][480/4184] Loss: 2.792275
[13/200][600/4184] Loss: 2.814903
[13/200][720/4184] Loss: 2.219153
[13/200][840/4184] Loss: 2.232554
[13/200][960/4184] Loss: 2.357638
[13/200][1080/4184] Loss: 2.174302
[13/200][1200/4184] Loss: 1.761841
[13/200][1320/4184] Loss: 2.472763
[13/200][1440/4184] Loss: 2.322558
[13/200][1560/4184] Loss: 1.984663
[13/200][1680/4184] Loss: 1.698622
[13/200][1800/4184] Loss: 2.275195
[13/200][1920/4184] Loss: 2.045503
[13/200][2040/4184] Loss: 1.947200
[13/200][2160/4184] Loss: 1.829886
[13/200][2280/4184] Loss: 1.977518
[13/200][2400/4184] Loss: 1.719286
[13/200][2520/4184] Loss: 2.382224
[13/200][2640/4184] Loss: 2.111312
[13/200][2760/4184] Loss: 3.000652
[13/200][2880/4184] Loss: 1.888879
[13/200][3000/4184] Loss: 1.997482
[13/200][3120/4184] Loss: 2.563385
[13/200][3240/4184] Loss: 1.843664
[13/200][3360/4184] Loss: 1.769816
[13/200][3480/4184] Loss: 3.243815
[13/200][3600/4184] Loss: 2.204137
[13/200][3720/4184] Loss: 1.747418
[13/200][3840/4184] Loss: 2.599828
[13/200][3960/4184] Loss: 3.654856
[13/200][4080/4184] Loss: 2.691544
[14/200][120/4184] Loss: 1.991423
[14/200][240/4184] Loss: 2.018392
[14/200][360/4184] Loss: 1.993415
[14/200][480/4184] Loss: 1.882526
[14/200][600/4184] Loss: 2.117551
[14/200][720/4184] Loss: 1.933464
[14/200][840/4184] Loss: 1.728293
[14/200][960/4184] Loss: 1.848747
[14/200][1080/4184] Loss: 2.871711
[14/200][1200/4184] Loss: 2.443910
[14/200][1320/4184] Loss: 1.753426
[14/200][1440/4184] Loss: 2.436102
[14/200][1560/4184] Loss: 2.895889
[14/200][1680/4184] Loss: 1.885005
[14/200][1800/4184] Loss: 2.460436
[14/200][1920/4184] Loss: 1.949155
[14/200][2040/4184] Loss: 1.843205
[14/200][2160/4184] Loss: 1.726413
[14/200][2280/4184] Loss: 2.061918
[14/200][2400/4184] Loss: 1.451113
[14/200][2520/4184] Loss: 1.751185
[14/200][2640/4184] Loss: 1.746402
[14/200][2760/4184] Loss: 1.591097
[14/200][2880/4184] Loss: 2.340058
[14/200][3000/4184] Loss: 1.751465
[14/200][3120/4184] Loss: 1.845076
[14/200][3240/4184] Loss: 1.751954
[14/200][3360/4184] Loss: 3.164689
[14/200][3480/4184] Loss: 1.798234
[14/200][3600/4184] Loss: 2.688323
[14/200][3720/4184] Loss: 1.848834
[14/200][3840/4184] Loss: 2.101911
[14/200][3960/4184] Loss: 1.550707
[14/200][4080/4184] Loss: 1.616173
[15/200][120/4184] Loss: 1.773297
[15/200][240/4184] Loss: 2.169602
[15/200][360/4184] Loss: 1.986758
[15/200][480/4184] Loss: 1.457069
[15/200][600/4184] Loss: 1.775461
[15/200][720/4184] Loss: 2.154342
[15/200][840/4184] Loss: 2.032545
[15/200][960/4184] Loss: 1.789543
[15/200][1080/4184] Loss: 1.647391
[15/200][1200/4184] Loss: 1.853034
[15/200][1320/4184] Loss: 1.878890
[15/200][1440/4184] Loss: 2.039017
[15/200][1560/4184] Loss: 1.770216
[15/200][1680/4184] Loss: 2.212803
[15/200][1800/4184] Loss: 1.882127
[15/200][1920/4184] Loss: 1.895856
[15/200][2040/4184] Loss: 1.443413
[15/200][2160/4184] Loss: 2.044161
[15/200][2280/4184] Loss: 2.337164
[15/200][2400/4184] Loss: 1.583912
[15/200][2520/4184] Loss: 1.468529
[15/200][2640/4184] Loss: 1.640900
[15/200][2760/4184] Loss: 1.892516
[15/200][2880/4184] Loss: 1.692952
[15/200][3000/4184] Loss: 1.584123
[15/200][3120/4184] Loss: 1.436608
[15/200][3240/4184] Loss: 1.665505
[15/200][3360/4184] Loss: 2.386375
[15/200][3480/4184] Loss: 1.781584
[15/200][3600/4184] Loss: 1.564938
[15/200][3720/4184] Loss: 2.049907
[15/200][3840/4184] Loss: 1.855774
[15/200][3960/4184] Loss: 2.001084
[15/200][4080/4184] Loss: 2.067252
Start validation set
-                    => v                   , gt: vorm Richter gesezt.
Total number of images in validation set:     1043
Test loss: 8.336497, accuracy: 0.227969
Character error rate mean: 0.1578; Character error rate sd: 0.6427
Word error rate mean: 0.4367; Word error rate sd: 0.3667
Start validation set
-----------------------------P----i--tt--ee---t-----.  I-------m--e----  -a----iin-- k----h---l--aa-----i-nn--s---   -G-----ww-------e---r--b---l-------------------------- => Pitet. Ime ain khlains Gwerbl, gt: Pitet. Ime ain khlains Gwerbl
----------------mmiii---tt-  -P-----e------rr------ccchh-----ttt---oo-----l----d-----tt-----s----   g------a------d--------n---ee-----r---   A-----r---bb----a----i----ttt- => mit Perchtoldts gadner Arbait, gt: mit Perchtoldts gadner Arbait
Total number of images in validation set:     2000
Test loss: 2.425907, accuracy: 0.556500
Character error rate mean: 0.0482; Character error rate sd: 0.1477
Word error rate mean: 0.1819; Word error rate sd: 0.2797
Saving epoch experiments/expr_READ_10Apr_binarize/netCRNN_15_4184.pth
[16/200][120/4184] Loss: 1.554278
[16/200][240/4184] Loss: 1.097139
[16/200][360/4184] Loss: 1.861452
[16/200][480/4184] Loss: 1.513117
[16/200][600/4184] Loss: 1.946867
[16/200][720/4184] Loss: 1.702742
[16/200][840/4184] Loss: 1.440296
[16/200][960/4184] Loss: 1.773815
[16/200][1080/4184] Loss: 1.364883
[16/200][1200/4184] Loss: 1.799718
[16/200][1320/4184] Loss: 1.679454
[16/200][1440/4184] Loss: 2.308099
[16/200][1560/4184] Loss: 1.895484
[16/200][1680/4184] Loss: 1.567594
[16/200][1800/4184] Loss: 1.487390
[16/200][1920/4184] Loss: 1.874429
[16/200][2040/4184] Loss: 1.262863
[16/200][2160/4184] Loss: 1.842639
[16/200][2280/4184] Loss: 1.794732
[16/200][2400/4184] Loss: 1.664688
[16/200][2520/4184] Loss: 1.465022
[16/200][2640/4184] Loss: 1.689911
[16/200][2760/4184] Loss: 1.888037
[16/200][2880/4184] Loss: 1.551671
[16/200][3000/4184] Loss: 1.462647
[16/200][3120/4184] Loss: 1.637356
[16/200][3240/4184] Loss: 2.243714
[16/200][3360/4184] Loss: 1.704970
[16/200][3480/4184] Loss: 1.214509
[16/200][3600/4184] Loss: 1.273923
[16/200][3720/4184] Loss: 1.609356
[16/200][3840/4184] Loss: 1.635831
[16/200][3960/4184] Loss: 1.823263
[16/200][4080/4184] Loss: 1.565701
[17/200][120/4184] Loss: 1.691071
[17/200][240/4184] Loss: 1.461704
[17/200][360/4184] Loss: 2.264841
[17/200][480/4184] Loss: 1.482605
[17/200][600/4184] Loss: 2.231184
[17/200][720/4184] Loss: 1.328131
[17/200][840/4184] Loss: 1.416533
[17/200][960/4184] Loss: 1.566251
[17/200][1080/4184] Loss: 1.171218
[17/200][1200/4184] Loss: 1.293466
[17/200][1320/4184] Loss: 1.389984
[17/200][1440/4184] Loss: 1.389835
[17/200][1560/4184] Loss: 1.233106
[17/200][1680/4184] Loss: 1.923231
[17/200][1800/4184] Loss: 1.666197
[17/200][1920/4184] Loss: 1.511552
[17/200][2040/4184] Loss: 1.265064
[17/200][2160/4184] Loss: 1.331061
[17/200][2280/4184] Loss: 1.784727
[17/200][2400/4184] Loss: 1.431060
[17/200][2520/4184] Loss: 1.075748
[17/200][2640/4184] Loss: 1.892983
[17/200][2760/4184] Loss: 1.709483
[17/200][2880/4184] Loss: 1.434418
[17/200][3000/4184] Loss: 1.610274
[17/200][3120/4184] Loss: 1.342379
[17/200][3240/4184] Loss: 1.230507
[17/200][3360/4184] Loss: 1.629942
[17/200][3480/4184] Loss: 1.421006
[17/200][3600/4184] Loss: 1.983628
[17/200][3720/4184] Loss: 1.511678
[17/200][3840/4184] Loss: 1.215065
[17/200][3960/4184] Loss: 1.395184
[17/200][4080/4184] Loss: 1.804323
[18/200][120/4184] Loss: 1.357406
[18/200][240/4184] Loss: 1.162035
[18/200][360/4184] Loss: 2.293079
[18/200][480/4184] Loss: 1.524763
[18/200][600/4184] Loss: 1.967597
[18/200][720/4184] Loss: 1.427454
[18/200][840/4184] Loss: 1.640978
[18/200][960/4184] Loss: 1.063158
[18/200][1080/4184] Loss: 1.343288
[18/200][1200/4184] Loss: 1.538155
[18/200][1320/4184] Loss: 1.372517
[18/200][1440/4184] Loss: 1.163022
[18/200][1560/4184] Loss: 1.374200
[18/200][1680/4184] Loss: 1.367690
[18/200][1800/4184] Loss: 1.534319
[18/200][1920/4184] Loss: 1.240992
[18/200][2040/4184] Loss: 1.292666
[18/200][2160/4184] Loss: 1.026106
[18/200][2280/4184] Loss: 1.432453
[18/200][2400/4184] Loss: 1.362722
[18/200][2520/4184] Loss: 1.070009
[18/200][2640/4184] Loss: 1.376509
[18/200][2760/4184] Loss: 1.166666
[18/200][2880/4184] Loss: 2.179841
[18/200][3000/4184] Loss: 1.245168
[18/200][3120/4184] Loss: 1.228756
[18/200][3240/4184] Loss: 1.190414
[18/200][3360/4184] Loss: 1.117121
[18/200][3480/4184] Loss: 2.028552
[18/200][3600/4184] Loss: 1.328302
[18/200][3720/4184] Loss: 1.454831
[18/200][3840/4184] Loss: 1.793303
[18/200][3960/4184] Loss: 1.116242
[18/200][4080/4184] Loss: 1.063473
[19/200][120/4184] Loss: 1.337922
[19/200][240/4184] Loss: 1.891325
[19/200][360/4184] Loss: 1.217179
[19/200][480/4184] Loss: 0.970431
[19/200][600/4184] Loss: 1.015784
[19/200][720/4184] Loss: 1.254003
[19/200][840/4184] Loss: 1.050564
[19/200][960/4184] Loss: 1.516447
[19/200][1080/4184] Loss: 1.051240
[19/200][1200/4184] Loss: 1.559731
[19/200][1320/4184] Loss: 1.116037
[19/200][1440/4184] Loss: 1.256498
[19/200][1560/4184] Loss: 0.942811
[19/200][1680/4184] Loss: 1.209826
[19/200][1800/4184] Loss: 1.234086
[19/200][1920/4184] Loss: 1.061024
[19/200][2040/4184] Loss: 1.111638
[19/200][2160/4184] Loss: 1.064838
[19/200][2280/4184] Loss: 1.590857
[19/200][2400/4184] Loss: 0.906978
[19/200][2520/4184] Loss: 1.412397
[19/200][2640/4184] Loss: 1.131067
[19/200][2760/4184] Loss: 1.290994
[19/200][2880/4184] Loss: 1.427958
[19/200][3000/4184] Loss: 1.101513
[19/200][3120/4184] Loss: 1.024001
[19/200][3240/4184] Loss: 1.508609
[19/200][3360/4184] Loss: 1.475084
[19/200][3480/4184] Loss: 1.150155
[19/200][3600/4184] Loss: 1.884661
[19/200][3720/4184] Loss: 1.046891
[19/200][3840/4184] Loss: 0.965679
[19/200][3960/4184] Loss: 0.998430
[19/200][4080/4184] Loss: 1.237067
[20/200][120/4184] Loss: 1.091164
[20/200][240/4184] Loss: 0.884582
[20/200][360/4184] Loss: 1.599249
[20/200][480/4184] Loss: 0.906488
[20/200][600/4184] Loss: 1.225394
[20/200][720/4184] Loss: 0.874596
[20/200][840/4184] Loss: 1.226489
[20/200][960/4184] Loss: 1.181779
[20/200][1080/4184] Loss: 1.606150
[20/200][1200/4184] Loss: 1.158338
[20/200][1320/4184] Loss: 1.101756
[20/200][1440/4184] Loss: 1.146789
[20/200][1560/4184] Loss: 1.210397
[20/200][1680/4184] Loss: nan
[20/200][1800/4184] Loss: 1.022722
[20/200][1920/4184] Loss: 0.946747
[20/200][2040/4184] Loss: 1.273048
[20/200][2160/4184] Loss: 1.090990
[20/200][2280/4184] Loss: nan
[20/200][2400/4184] Loss: 0.860758
[20/200][2520/4184] Loss: 1.110362
[20/200][2640/4184] Loss: 1.353535
[20/200][2760/4184] Loss: 1.151811
[20/200][2880/4184] Loss: 1.058509
[20/200][3000/4184] Loss: 0.984659
[20/200][3120/4184] Loss: 1.073379
[20/200][3240/4184] Loss: 0.977384
[20/200][3360/4184] Loss: 0.919862
[20/200][3480/4184] Loss: 1.206687
[20/200][3600/4184] Loss: 0.929735
[20/200][3720/4184] Loss: 1.160874
[20/200][3840/4184] Loss: 1.135472
[20/200][3960/4184] Loss: 0.829012
[20/200][4080/4184] Loss: 1.523439
Start validation set
-                    => d                   , gt: der Rebelln Nämen   
Total number of images in validation set:     1043
Test loss: 9.658883, accuracy: 0.208812
Character error rate mean: 0.1696; Character error rate sd: 0.6159
Word error rate mean: 0.4598; Word error rate sd: 0.4166
Start validation set
-Z----a-----l----ūūū---nn-gg------   g----e------ū---o---l---g----tt---,,-   g-----ee--gg-----e---b-----e-----nn-- => Zalūng geūolgt, gegeben, gt: Zalūng geūolgt, gegeben
------w------------------------------ee------------rr-------------dd---------------ee---------------nn--------,--- => werden,             , gt: werden,             
Total number of images in validation set:     2000
Test loss: 1.543140, accuracy: 0.691500
Character error rate mean: 0.0391; Character error rate sd: 0.1942
Word error rate mean: 0.1364; Word error rate sd: 0.2841
Saving epoch experiments/expr_READ_10Apr_binarize/netCRNN_20_4184.pth
[21/200][120/4184] Loss: 0.892302
[21/200][240/4184] Loss: 1.046653
[21/200][360/4184] Loss: 1.479532
[21/200][480/4184] Loss: 1.069047
[21/200][600/4184] Loss: 1.050756
[21/200][720/4184] Loss: 0.991783
[21/200][840/4184] Loss: 0.986262
[21/200][960/4184] Loss: 0.880945
[21/200][1080/4184] Loss: 1.222917
[21/200][1200/4184] Loss: 1.084698
[21/200][1320/4184] Loss: nan
[21/200][1440/4184] Loss: 0.764098
[21/200][1560/4184] Loss: 0.955660
[21/200][1680/4184] Loss: 1.586620
[21/200][1800/4184] Loss: 1.203385
[21/200][1920/4184] Loss: 1.020681
[21/200][2040/4184] Loss: 1.056541
[21/200][2160/4184] Loss: 0.825466
[21/200][2280/4184] Loss: 0.796867
[21/200][2400/4184] Loss: 0.757153
[21/200][2520/4184] Loss: 0.984763
[21/200][2640/4184] Loss: 1.038127
[21/200][2760/4184] Loss: nan
[21/200][2880/4184] Loss: 0.955403
[21/200][3000/4184] Loss: 0.831035
[21/200][3120/4184] Loss: 2.519479
[21/200][3240/4184] Loss: 0.896841
[21/200][3360/4184] Loss: 1.043744
[21/200][3480/4184] Loss: 1.009754
[21/200][3600/4184] Loss: 0.878616
[21/200][3720/4184] Loss: 0.809092
[21/200][3840/4184] Loss: 0.989652
[21/200][3960/4184] Loss: 1.105274
[21/200][4080/4184] Loss: 0.816894
[22/200][120/4184] Loss: 0.895346
[22/200][240/4184] Loss: 0.913879
[22/200][360/4184] Loss: 1.108489
[22/200][480/4184] Loss: 1.217991
[22/200][600/4184] Loss: 0.988893
[22/200][720/4184] Loss: 1.448452
[22/200][840/4184] Loss: 1.104784
[22/200][960/4184] Loss: 0.924608
[22/200][1080/4184] Loss: 1.137049
[22/200][1200/4184] Loss: 0.951963
[22/200][1320/4184] Loss: 0.994195
[22/200][1440/4184] Loss: 0.887823
[22/200][1560/4184] Loss: 0.795604
[22/200][1680/4184] Loss: 1.068293
[22/200][1800/4184] Loss: 0.875941
[22/200][1920/4184] Loss: 0.912805
[22/200][2040/4184] Loss: 0.861834
[22/200][2160/4184] Loss: 0.999404
[22/200][2280/4184] Loss: 0.859288
[22/200][2400/4184] Loss: 0.674111
[22/200][2520/4184] Loss: 1.157591
[22/200][2640/4184] Loss: 0.914764
[22/200][2760/4184] Loss: 0.628332
[22/200][2880/4184] Loss: 1.001947
[22/200][3000/4184] Loss: 0.899106
[22/200][3120/4184] Loss: 1.428837
[22/200][3240/4184] Loss: 1.104998
[22/200][3360/4184] Loss: 0.881734
[22/200][3480/4184] Loss: 0.897676
[22/200][3600/4184] Loss: 1.148285
[22/200][3720/4184] Loss: 0.883524
[22/200][3840/4184] Loss: 0.730944
[22/200][3960/4184] Loss: 1.802504
[22/200][4080/4184] Loss: 0.813911
[23/200][120/4184] Loss: 1.297512
[23/200][240/4184] Loss: 0.999071
[23/200][360/4184] Loss: 1.720346
[23/200][480/4184] Loss: 0.832060
[23/200][600/4184] Loss: 0.974440
[23/200][720/4184] Loss: 0.993945
[23/200][840/4184] Loss: 0.988788
[23/200][960/4184] Loss: 1.641892
[23/200][1080/4184] Loss: 1.077966
[23/200][1200/4184] Loss: 1.385614
[23/200][1320/4184] Loss: 0.802367
[23/200][1440/4184] Loss: 0.896303
[23/200][1560/4184] Loss: 0.948542
[23/200][1680/4184] Loss: 1.228393
[23/200][1800/4184] Loss: 0.910690
[23/200][1920/4184] Loss: 0.927245
[23/200][2040/4184] Loss: 1.219139
[23/200][2160/4184] Loss: 1.104819
[23/200][2280/4184] Loss: 1.069920
[23/200][2400/4184] Loss: 0.717597
[23/200][2520/4184] Loss: 0.531911
[23/200][2640/4184] Loss: 0.827804
[23/200][2760/4184] Loss: 0.803283
[23/200][2880/4184] Loss: 0.838443
[23/200][3000/4184] Loss: 0.976013
[23/200][3120/4184] Loss: 0.881388
[23/200][3240/4184] Loss: 1.054477
[23/200][3360/4184] Loss: 1.183879
[23/200][3480/4184] Loss: 0.708747
[23/200][3600/4184] Loss: 0.964286
[23/200][3720/4184] Loss: 0.808033
[23/200][3840/4184] Loss: 0.898593
[23/200][3960/4184] Loss: 0.645706
[23/200][4080/4184] Loss: 0.881300
[24/200][120/4184] Loss: 0.877189
[24/200][240/4184] Loss: 0.711269
[24/200][360/4184] Loss: 0.907847
[24/200][480/4184] Loss: 0.654717
[24/200][600/4184] Loss: 0.616693
[24/200][720/4184] Loss: 0.724198
[24/200][840/4184] Loss: 0.828260
[24/200][960/4184] Loss: 1.142104
[24/200][1080/4184] Loss: 0.920463
[24/200][1200/4184] Loss: 0.698674
[24/200][1320/4184] Loss: 0.787111
[24/200][1440/4184] Loss: 1.373215
[24/200][1560/4184] Loss: 0.798150
[24/200][1680/4184] Loss: 0.928159
[24/200][1800/4184] Loss: 0.820726
[24/200][1920/4184] Loss: 0.654602
[24/200][2040/4184] Loss: 0.712785
[24/200][2160/4184] Loss: 0.669772
[24/200][2280/4184] Loss: 0.635614
[24/200][2400/4184] Loss: 1.025280
[24/200][2520/4184] Loss: 1.747415
[24/200][2640/4184] Loss: 0.748509
[24/200][2760/4184] Loss: 0.705885
[24/200][2880/4184] Loss: 1.438324
[24/200][3000/4184] Loss: 0.664575
[24/200][3120/4184] Loss: 0.797161
[24/200][3240/4184] Loss: 0.889478
[24/200][3360/4184] Loss: 0.791822
[24/200][3480/4184] Loss: 0.788722
[24/200][3600/4184] Loss: 1.175387
[24/200][3720/4184] Loss: 1.104754
[24/200][3840/4184] Loss: 0.623718
[24/200][3960/4184] Loss: 1.259804
[24/200][4080/4184] Loss: 0.761512
[25/200][120/4184] Loss: 0.766079
[25/200][240/4184] Loss: 1.613043
[25/200][360/4184] Loss: 0.535415
[25/200][480/4184] Loss: 0.747512
[25/200][600/4184] Loss: 0.839964
[25/200][720/4184] Loss: 0.555170
[25/200][840/4184] Loss: 0.853805
[25/200][960/4184] Loss: 0.818508
[25/200][1080/4184] Loss: 0.547759
[25/200][1200/4184] Loss: 0.738162
[25/200][1320/4184] Loss: 0.839271
[25/200][1440/4184] Loss: 1.606868
[25/200][1560/4184] Loss: 0.643589
[25/200][1680/4184] Loss: 0.700591
[25/200][1800/4184] Loss: 1.757234
[25/200][1920/4184] Loss: 0.873678
[25/200][2040/4184] Loss: 0.797252
[25/200][2160/4184] Loss: 0.771115
[25/200][2280/4184] Loss: 0.742435
[25/200][2400/4184] Loss: 0.605567
[25/200][2520/4184] Loss: 1.232835
[25/200][2640/4184] Loss: 0.693648
[25/200][2760/4184] Loss: 0.563259
[25/200][2880/4184] Loss: 0.705843
[25/200][3000/4184] Loss: 1.039084
[25/200][3120/4184] Loss: 0.834407
[25/200][3240/4184] Loss: 0.946691
[25/200][3360/4184] Loss: 0.719092
[25/200][3480/4184] Loss: 0.787648
[25/200][3600/4184] Loss: 0.776040
[25/200][3720/4184] Loss: 0.790983
[25/200][3840/4184] Loss: 0.615060
[25/200][3960/4184] Loss: 0.968991
[25/200][4080/4184] Loss: 0.838592
Start validation set
-                    => C                   , gt: Carl von Frindeshaim
Total number of images in validation set:     1043
Test loss: 10.201816, accuracy: 0.244253
Character error rate mean: 0.1695; Character error rate sd: 0.7295
Word error rate mean: 0.4584; Word error rate sd: 0.4423
Start validation set
------------m---a-----n---    kk---hh--a------iii----n--ee---nn--   v------------m---b---gg-----a--------nn--------nn--g------- => man khainen vmbganng, gt: man khainen vmbganng
------nn---eh--------------m---ee-----n----,-----   -d--------ii--e----   ss---a-------ccchhh---e-----n-----   a----------nn--- => nehmen, die sachen an, gt: nehmen, die sachen an
Total number of images in validation set:     2000
Test loss: 1.117946, accuracy: 0.748500
Character error rate mean: 0.0351; Character error rate sd: 0.2981
Word error rate mean: 0.0998; Word error rate sd: 0.2397
Saving epoch experiments/expr_READ_10Apr_binarize/netCRNN_25_4184.pth
[26/200][120/4184] Loss: 0.615431
[26/200][240/4184] Loss: 0.592954
[26/200][360/4184] Loss: 0.550607
[26/200][480/4184] Loss: 0.641251
[26/200][600/4184] Loss: 0.840704
[26/200][720/4184] Loss: 0.769895
[26/200][840/4184] Loss: 0.662073
[26/200][960/4184] Loss: 0.740545
[26/200][1080/4184] Loss: 0.623448
[26/200][1200/4184] Loss: 0.720796
[26/200][1320/4184] Loss: 1.197750
[26/200][1440/4184] Loss: 0.716831
[26/200][1560/4184] Loss: 1.068054
[26/200][1680/4184] Loss: nan
[26/200][1800/4184] Loss: 0.511885
[26/200][1920/4184] Loss: 0.531783
[26/200][2040/4184] Loss: 0.921975
[26/200][2160/4184] Loss: 0.730261
[26/200][2280/4184] Loss: 0.780521
[26/200][2400/4184] Loss: 0.737677
[26/200][2520/4184] Loss: 0.582839
[26/200][2640/4184] Loss: 0.591029
[26/200][2760/4184] Loss: 0.730131
[26/200][2880/4184] Loss: 0.629439
[26/200][3000/4184] Loss: 0.616724
[26/200][3120/4184] Loss: 0.802160
[26/200][3240/4184] Loss: 0.601738
[26/200][3360/4184] Loss: 0.594445
[26/200][3480/4184] Loss: 0.697836
[26/200][3600/4184] Loss: 0.968840
[26/200][3720/4184] Loss: 0.868681
[26/200][3840/4184] Loss: 0.600728
[26/200][3960/4184] Loss: 1.236601
[26/200][4080/4184] Loss: 1.339115
[27/200][120/4184] Loss: 0.646817
[27/200][240/4184] Loss: 0.660654
[27/200][360/4184] Loss: 0.711631
[27/200][480/4184] Loss: 0.872295
[27/200][600/4184] Loss: 0.732227
[27/200][720/4184] Loss: 1.307487
[27/200][840/4184] Loss: 0.633546
[27/200][960/4184] Loss: 0.605531
[27/200][1080/4184] Loss: 0.710823
[27/200][1200/4184] Loss: 0.733077
[27/200][1320/4184] Loss: 0.499116
[27/200][1440/4184] Loss: 0.789857
[27/200][1560/4184] Loss: 0.660427
[27/200][1680/4184] Loss: 0.563099
[27/200][1800/4184] Loss: 0.758732
[27/200][1920/4184] Loss: 0.639854
[27/200][2040/4184] Loss: 0.752450
[27/200][2160/4184] Loss: 0.806489
[27/200][2280/4184] Loss: 0.525277
[27/200][2400/4184] Loss: 0.631702
[27/200][2520/4184] Loss: 0.550420
[27/200][2640/4184] Loss: 0.643907
[27/200][2760/4184] Loss: 0.501183
[27/200][2880/4184] Loss: 0.747307
[27/200][3000/4184] Loss: 0.664642
[27/200][3120/4184] Loss: 0.460117
[27/200][3240/4184] Loss: 0.707831
[27/200][3360/4184] Loss: 0.529289
[27/200][3480/4184] Loss: 0.558149
[27/200][3600/4184] Loss: 0.691496
[27/200][3720/4184] Loss: 0.885372
[27/200][3840/4184] Loss: 0.618747
[27/200][3960/4184] Loss: 0.406769
[27/200][4080/4184] Loss: 0.610858
[28/200][120/4184] Loss: 0.703821
[28/200][240/4184] Loss: 0.491449
[28/200][360/4184] Loss: 0.940406
[28/200][480/4184] Loss: 0.712718
[28/200][600/4184] Loss: 0.812430
[28/200][720/4184] Loss: 0.514626
[28/200][840/4184] Loss: 0.644165
[28/200][960/4184] Loss: 0.569411
[28/200][1080/4184] Loss: 0.683023
[28/200][1200/4184] Loss: 0.792962
[28/200][1320/4184] Loss: 0.725073
[28/200][1440/4184] Loss: 0.581419
[28/200][1560/4184] Loss: 0.390350
[28/200][1680/4184] Loss: 0.532319
[28/200][1800/4184] Loss: 0.654809
[28/200][1920/4184] Loss: 0.565480
[28/200][2040/4184] Loss: 0.742119
[28/200][2160/4184] Loss: 0.662229
[28/200][2280/4184] Loss: 0.517989
[28/200][2400/4184] Loss: 0.783286
[28/200][2520/4184] Loss: 0.607101
[28/200][2640/4184] Loss: 0.791198
[28/200][2760/4184] Loss: 0.532423
[28/200][2880/4184] Loss: 0.635762
[28/200][3000/4184] Loss: 0.631980
[28/200][3120/4184] Loss: 1.238707
[28/200][3240/4184] Loss: 0.583752
[28/200][3360/4184] Loss: 0.518497
[28/200][3480/4184] Loss: 0.620498
[28/200][3600/4184] Loss: 0.700826
[28/200][3720/4184] Loss: 0.743915
[28/200][3840/4184] Loss: 0.515337
[28/200][3960/4184] Loss: 0.675054
[28/200][4080/4184] Loss: 1.047816
[29/200][120/4184] Loss: 0.682414
[29/200][240/4184] Loss: 0.639052
[29/200][360/4184] Loss: 1.004369
[29/200][480/4184] Loss: 0.549064
[29/200][600/4184] Loss: 0.571198
[29/200][720/4184] Loss: 0.654339
[29/200][840/4184] Loss: 0.665624
[29/200][960/4184] Loss: 0.708765
[29/200][1080/4184] Loss: 0.599650
[29/200][1200/4184] Loss: 0.705084
[29/200][1320/4184] Loss: 0.549201
[29/200][1440/4184] Loss: 1.370163
[29/200][1560/4184] Loss: 0.545794
[29/200][1680/4184] Loss: 0.479479
[29/200][1800/4184] Loss: 0.545222
[29/200][1920/4184] Loss: 0.728271
[29/200][2040/4184] Loss: 0.530622
[29/200][2160/4184] Loss: 0.489010
[29/200][2280/4184] Loss: 0.772139
[29/200][2400/4184] Loss: 0.480144
[29/200][2520/4184] Loss: 0.620537
[29/200][2640/4184] Loss: 0.568475
[29/200][2760/4184] Loss: 0.391999
[29/200][2880/4184] Loss: 0.412709
[29/200][3000/4184] Loss: 0.530829
[29/200][3120/4184] Loss: 0.776200
[29/200][3240/4184] Loss: 0.638535
[29/200][3360/4184] Loss: 0.570475
[29/200][3480/4184] Loss: 0.887482
[29/200][3600/4184] Loss: 0.444835
[29/200][3720/4184] Loss: 0.602593
[29/200][3840/4184] Loss: 0.567557
[29/200][3960/4184] Loss: 0.628569
[29/200][4080/4184] Loss: 0.757594
[30/200][120/4184] Loss: 0.940492
[30/200][240/4184] Loss: 0.569359
[30/200][360/4184] Loss: 0.569243
[30/200][480/4184] Loss: 0.509407
[30/200][600/4184] Loss: 0.637268
[30/200][720/4184] Loss: 0.699511
[30/200][840/4184] Loss: 0.472339
[30/200][960/4184] Loss: 0.636429
[30/200][1080/4184] Loss: 0.538601
[30/200][1200/4184] Loss: 0.704402
[30/200][1320/4184] Loss: 0.529734
[30/200][1440/4184] Loss: 1.230295
[30/200][1560/4184] Loss: 0.867795
[30/200][1680/4184] Loss: 0.615806
[30/200][1800/4184] Loss: 1.007917
[30/200][1920/4184] Loss: 1.085116
[30/200][2040/4184] Loss: 0.514901
[30/200][2160/4184] Loss: 0.662405
[30/200][2280/4184] Loss: 0.499053
[30/200][2400/4184] Loss: 0.372402
[30/200][2520/4184] Loss: 0.511107
[30/200][2640/4184] Loss: 0.598577
[30/200][2760/4184] Loss: 0.569363
[30/200][2880/4184] Loss: 0.614229
[30/200][3000/4184] Loss: 0.749613
[30/200][3120/4184] Loss: 0.666186
[30/200][3240/4184] Loss: 0.401875
[30/200][3360/4184] Loss: 0.713084
[30/200][3480/4184] Loss: 1.047421
[30/200][3600/4184] Loss: 0.557180
[30/200][3720/4184] Loss: 0.533229
[30/200][3840/4184] Loss: 0.584118
[30/200][3960/4184] Loss: 0.440652
[30/200][4080/4184] Loss: 0.787303
Start validation set
-                    => E                   , gt: Es soll ainer Loblichen
Total number of images in validation set:     1043
Test loss: 10.505205, accuracy: 0.252874
Character error rate mean: 0.1349; Character error rate sd: 0.7055
Word error rate mean: 0.4111; Word error rate sd: 0.4087
Start validation set
---P-------ii-----t----e-----tt-----.--   I------------mm-ee----   v-----o----n----   d-----e----------mm--    -R--------ii---f---ll---a------ūū------nn-- => Pitet. Ime von dem Riflaūn, gt: Pitet. Ime von dem Riflaūn
-----------P-----a-------ccchhh---.-   bb-----e----ii---   -d-----ee----------m----   M-----------a-------tt--hh--ee-------ū---ss----ee-------n----------- => Pach. bei dem Matheūsen, gt: Pach. bei dem Matheūsen
Total number of images in validation set:     2000
Test loss: 0.831411, accuracy: 0.805500
Character error rate mean: 0.0175; Character error rate sd: 0.0796
Word error rate mean: 0.0707; Word error rate sd: 0.1966
Saving epoch experiments/expr_READ_10Apr_binarize/netCRNN_30_4184.pth
[31/200][120/4184] Loss: 0.570277
[31/200][240/4184] Loss: 0.595854
[31/200][360/4184] Loss: 0.475579
[31/200][480/4184] Loss: 0.737275
[31/200][600/4184] Loss: 0.698550
[31/200][720/4184] Loss: 0.551778
[31/200][840/4184] Loss: 0.608494
[31/200][960/4184] Loss: 0.432083
[31/200][1080/4184] Loss: 0.617876
[31/200][1200/4184] Loss: 0.484422
[31/200][1320/4184] Loss: 0.447821
[31/200][1440/4184] Loss: 0.572426
[31/200][1560/4184] Loss: 0.490255
[31/200][1680/4184] Loss: 0.780710
[31/200][1800/4184] Loss: 0.627674
[31/200][1920/4184] Loss: 0.477131
[31/200][2040/4184] Loss: 0.590274
[31/200][2160/4184] Loss: 0.562744
[31/200][2280/4184] Loss: 0.425182
[31/200][2400/4184] Loss: 0.426655
[31/200][2520/4184] Loss: 0.460205
[31/200][2640/4184] Loss: 0.447387
[31/200][2760/4184] Loss: 0.669917
[31/200][2880/4184] Loss: 0.585441
[31/200][3000/4184] Loss: 0.520212
[31/200][3120/4184] Loss: 0.641990
[31/200][3240/4184] Loss: nan
[31/200][3360/4184] Loss: nan
[31/200][3480/4184] Loss: 1.137175
[31/200][3600/4184] Loss: 0.649373
[31/200][3720/4184] Loss: 0.608988
[31/200][3840/4184] Loss: 0.724353
[31/200][3960/4184] Loss: 0.630317
[31/200][4080/4184] Loss: 0.502579
[32/200][120/4184] Loss: 0.614725
[32/200][240/4184] Loss: 0.444272
[32/200][360/4184] Loss: 0.417647
[32/200][480/4184] Loss: 0.494724
[32/200][600/4184] Loss: 0.569986
[32/200][720/4184] Loss: 0.504403
[32/200][840/4184] Loss: 0.618258
[32/200][960/4184] Loss: 0.675969
[32/200][1080/4184] Loss: 0.525576
[32/200][1200/4184] Loss: 0.402361
[32/200][1320/4184] Loss: 0.521661
[32/200][1440/4184] Loss: 0.440613
[32/200][1560/4184] Loss: 0.479655
[32/200][1680/4184] Loss: 0.479097
[32/200][1800/4184] Loss: 0.562435
[32/200][1920/4184] Loss: 0.460954
[32/200][2040/4184] Loss: 0.441365
[32/200][2160/4184] Loss: 0.462745
[32/200][2280/4184] Loss: 0.370556
[32/200][2400/4184] Loss: 0.614444
[32/200][2520/4184] Loss: 0.598432
[32/200][2640/4184] Loss: 0.567575
[32/200][2760/4184] Loss: 0.440162
[32/200][2880/4184] Loss: 0.957020
[32/200][3000/4184] Loss: 0.412934
[32/200][3120/4184] Loss: 0.674671
[32/200][3240/4184] Loss: 0.492163
[32/200][3360/4184] Loss: 0.488457
[32/200][3480/4184] Loss: 0.567159
[32/200][3600/4184] Loss: 0.432843
[32/200][3720/4184] Loss: 0.534192
[32/200][3840/4184] Loss: 0.506509
[32/200][3960/4184] Loss: 0.637553
[32/200][4080/4184] Loss: 0.583879
[33/200][120/4184] Loss: 0.396330
[33/200][240/4184] Loss: 0.530587
[33/200][360/4184] Loss: 0.396731
[33/200][480/4184] Loss: 0.525229
[33/200][600/4184] Loss: 0.543631
[33/200][720/4184] Loss: 0.545449
[33/200][840/4184] Loss: 0.698350
[33/200][960/4184] Loss: 0.411565
[33/200][1080/4184] Loss: 0.445199
[33/200][1200/4184] Loss: 0.531980
[33/200][1320/4184] Loss: 0.429745
[33/200][1440/4184] Loss: 0.425844
[33/200][1560/4184] Loss: 0.377987
[33/200][1680/4184] Loss: 0.574411
[33/200][1800/4184] Loss: 0.493244
[33/200][1920/4184] Loss: 0.402697
[33/200][2040/4184] Loss: 0.495205
[33/200][2160/4184] Loss: 0.509558
[33/200][2280/4184] Loss: 0.472664
[33/200][2400/4184] Loss: 0.537909
[33/200][2520/4184] Loss: 0.411579
[33/200][2640/4184] Loss: 0.964177
[33/200][2760/4184] Loss: 0.553367
[33/200][2880/4184] Loss: 0.400948
[33/200][3000/4184] Loss: 0.320885
[33/200][3120/4184] Loss: 0.633128
[33/200][3240/4184] Loss: 0.450229
[33/200][3360/4184] Loss: 0.427048
[33/200][3480/4184] Loss: 0.482154
[33/200][3600/4184] Loss: 0.473324
[33/200][3720/4184] Loss: 0.482737
[33/200][3840/4184] Loss: 0.527469
[33/200][3960/4184] Loss: 0.572871
[33/200][4080/4184] Loss: 0.547100
[34/200][120/4184] Loss: 0.517875
[34/200][240/4184] Loss: 0.341216
[34/200][360/4184] Loss: 0.407354
[34/200][480/4184] Loss: 0.432731
[34/200][600/4184] Loss: 0.677769
[34/200][720/4184] Loss: 0.602026
[34/200][840/4184] Loss: 0.495999
[34/200][960/4184] Loss: 0.709843
[34/200][1080/4184] Loss: 0.554343
[34/200][1200/4184] Loss: 0.637779
[34/200][1320/4184] Loss: 0.288565
[34/200][1440/4184] Loss: 0.490206
[34/200][1560/4184] Loss: 0.362632
[34/200][1680/4184] Loss: 0.481144
[34/200][1800/4184] Loss: 0.616401
[34/200][1920/4184] Loss: 0.599561
[34/200][2040/4184] Loss: 0.571318
[34/200][2160/4184] Loss: 0.430383
[34/200][2280/4184] Loss: 0.317892
[34/200][2400/4184] Loss: 0.580337
[34/200][2520/4184] Loss: 0.615608
[34/200][2640/4184] Loss: 0.693393
[34/200][2760/4184] Loss: 0.339369
[34/200][2880/4184] Loss: 0.674622
[34/200][3000/4184] Loss: 0.421674
[34/200][3120/4184] Loss: 0.285229
[34/200][3240/4184] Loss: 0.552166
[34/200][3360/4184] Loss: 0.699383
[34/200][3480/4184] Loss: 0.479511
[34/200][3600/4184] Loss: 0.443034
[34/200][3720/4184] Loss: 0.326272
[34/200][3840/4184] Loss: 0.562430
[34/200][3960/4184] Loss: 0.451307
[34/200][4080/4184] Loss: 0.266598
[35/200][120/4184] Loss: 0.410062
[35/200][240/4184] Loss: 0.382653
[35/200][360/4184] Loss: 0.401404
[35/200][480/4184] Loss: 0.499474
[35/200][600/4184] Loss: 0.461310
[35/200][720/4184] Loss: 0.432576
[35/200][840/4184] Loss: 0.477813
[35/200][960/4184] Loss: 0.718969
[35/200][1080/4184] Loss: 0.464599
[35/200][1200/4184] Loss: 0.385873
[35/200][1320/4184] Loss: 0.558657
[35/200][1440/4184] Loss: 0.588811
[35/200][1560/4184] Loss: 0.356490
[35/200][1680/4184] Loss: 0.470210
[35/200][1800/4184] Loss: 0.433707
[35/200][1920/4184] Loss: 0.351853
[35/200][2040/4184] Loss: 0.478485
[35/200][2160/4184] Loss: 0.344161
[35/200][2280/4184] Loss: 0.648386
[35/200][2400/4184] Loss: 0.287777
[35/200][2520/4184] Loss: 0.361579
[35/200][2640/4184] Loss: 0.365025
[35/200][2760/4184] Loss: 0.424174
[35/200][2880/4184] Loss: 0.466659
[35/200][3000/4184] Loss: 0.544578
[35/200][3120/4184] Loss: 0.405752
[35/200][3240/4184] Loss: 0.366247
[35/200][3360/4184] Loss: 0.614063
[35/200][3480/4184] Loss: 0.606721
[35/200][3600/4184] Loss: 0.391318
[35/200][3720/4184] Loss: 0.659992
[35/200][3840/4184] Loss: 0.627154
[35/200][3960/4184] Loss: 0.377975
[35/200][4080/4184] Loss: 0.557810
Start validation set
-                    => N                   , gt: Niderhaūs genant. bis in
Total number of images in validation set:     1043
Test loss: 12.091518, accuracy: 0.207854
Character error rate mean: 0.1854; Character error rate sd: 0.7942
Word error rate mean: 0.4714; Word error rate sd: 0.4428
Start validation set
---h--------a------------nn------nn----s-------   P----------r-------ūūū---g--------g-----------rr---------e-------ūū-------ttt-e------rr------,,----   ----------mmmmmiiiii------t-------- => hanns Prūggreūter, mit, gt: hanns Prūggreūter, mit
------------------------------dd------ee-----nn---  h---e-----rr------r------n---   -a----iiii---nn-ee--ss---  EE----r-----sss----a-----------mm--ee-----n--------------------------------- => den herrn aines Ersamen, gt: den herrn aines Ersamen
Total number of images in validation set:     2000
Test loss: 0.786090, accuracy: 0.806500
Character error rate mean: 0.0210; Character error rate sd: 0.1148
Word error rate mean: 0.0790; Word error rate sd: 0.2328
Saving epoch experiments/expr_READ_10Apr_binarize/netCRNN_35_4184.pth
[36/200][120/4184] Loss: 0.458439
[36/200][240/4184] Loss: 0.407196
[36/200][360/4184] Loss: 0.403459
[36/200][480/4184] Loss: 0.477207
[36/200][600/4184] Loss: 0.367291
[36/200][720/4184] Loss: 0.582398
[36/200][840/4184] Loss: 0.473761
[36/200][960/4184] Loss: 0.476999
[36/200][1080/4184] Loss: 0.421807
[36/200][1200/4184] Loss: 0.496417
[36/200][1320/4184] Loss: 0.448743
[36/200][1440/4184] Loss: 0.975435
[36/200][1560/4184] Loss: 0.822532
[36/200][1680/4184] Loss: 0.397038
[36/200][1800/4184] Loss: 0.323523
[36/200][1920/4184] Loss: 0.391774
[36/200][2040/4184] Loss: 0.578935
[36/200][2160/4184] Loss: 0.464468
[36/200][2280/4184] Loss: 0.547409
[36/200][2400/4184] Loss: 0.552102
[36/200][2520/4184] Loss: 0.425012
[36/200][2640/4184] Loss: 0.631111
[36/200][2760/4184] Loss: 0.404789
[36/200][2880/4184] Loss: 0.366029
[36/200][3000/4184] Loss: 0.368366
[36/200][3120/4184] Loss: 0.485009
[36/200][3240/4184] Loss: 1.190664
[36/200][3360/4184] Loss: 0.356570
Running with options: Namespace(adadelta=False, adam=False, alphabet='0123456789abcdefghijklmnopqrstuvwxyz', batchSize=2, beta1=0.5, binarize=False, crnn='', cuda=True, displayInterval=120, experiment='experiments/expr_READ_14Apr_prepareforgit', imgH=60, imgW=240, keep_ratio=True, lr=0.0001, n_test_disp=10, ngpu=1, nh=256, niter=200, random_sample=False, saveEpoch=5, test_file='test_file', test_icfhr=False, trainroot='/deep_data/nephi/data/lmdb_read_bin/train', valEpoch=5, valroot='/deep_data/nephi/data/lmdb_read_bin/val', workers=10)
Random Seed:  1157
This is the alphabet:
40FreihatnūfsRb¬:vdgüomw.DlENBcLkPzAOuVȳGSCIH19Tj62,W5KōZpäÿMJ38x()āÖqß̄—öU7̈ē-Q<>/¾yY+ 
Your neural network: DataParallel(
  (module): CRNN(
    (cnn): Sequential(
      (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu0): ReLU(inplace)
      (pooling0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1): ReLU(inplace)
      (pooling1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (relu2): ReLU(inplace)
      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3): ReLU(inplace)
      (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu4): ReLU(inplace)
      (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5): ReLU(inplace)
      (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))
      (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu6): ReLU(inplace)
    )
    (rnn): Sequential(
      (0): BidirectionalLSTM(
        (rnn): LSTM(512, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): BidirectionalLSTM(
        (rnn): LSTM(256, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=89, bias=True)
      )
    )
  )
)
Starting training...
Running with options: Namespace(adadelta=False, adam=False, alphabet='0123456789abcdefghijklmnopqrstuvwxyz', batchSize=2, beta1=0.5, binarize=False, crnn='', cuda=True, displayInterval=120, experiment='experiments/expr_READ_14Apr_prepareforgit', imgH=60, imgW=240, keep_ratio=True, lr=0.0001, n_test_disp=10, ngpu=1, nh=256, niter=200, random_sample=False, saveEpoch=5, test_file='test_file', test_icfhr=False, trainroot='/deep_data/nephi/data/lmdb_read_bin/train', valEpoch=5, valroot='/deep_data/nephi/data/lmdb_read_bin/val', workers=10)
Random Seed:  6054
This is the alphabet:
40FreihatnūfsRb¬:vdgüomw.DlENBcLkPzAOuVȳGSCIH19Tj62,W5KōZpäÿMJ38x()āÖqß̄—öU7̈ē-Q<>/¾yY+ 
Your neural network: DataParallel(
  (module): CRNN(
    (cnn): Sequential(
      (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu0): ReLU(inplace)
      (pooling0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1): ReLU(inplace)
      (pooling1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (relu2): ReLU(inplace)
      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3): ReLU(inplace)
      (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu4): ReLU(inplace)
      (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5): ReLU(inplace)
      (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))
      (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu6): ReLU(inplace)
    )
    (rnn): Sequential(
      (0): BidirectionalLSTM(
        (rnn): LSTM(512, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): BidirectionalLSTM(
        (rnn): LSTM(256, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=89, bias=True)
      )
    )
  )
)
Starting training...
Running with options: Namespace(adadelta=False, adam=False, alphabet='0123456789abcdefghijklmnopqrstuvwxyz', batchSize=2, beta1=0.5, binarize=False, crnn='', cuda=True, displayInterval=120, experiment='experiments/expr_READ_14Apr_prepareforgit', imgH=60, imgW=240, keep_ratio=True, lr=0.0001, n_test_disp=10, ngpu=1, nh=256, niter=200, random_sample=False, saveEpoch=5, test_file='test_file', test_icfhr=False, trainroot='/deep_data/nephi/data/lmdb_read_bin/train', valEpoch=5, valroot='/deep_data/nephi/data/lmdb_read_bin/val', workers=10)
Random Seed:  6115
This is the alphabet:
40FreihatnūfsRb¬:vdgüomw.DlENBcLkPzAOuVȳGSCIH19Tj62,W5KōZpäÿMJ38x()āÖqß̄—öU7̈ē-Q<>/¾yY+ 
Your neural network: DataParallel(
  (module): CRNN(
    (cnn): Sequential(
      (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu0): ReLU(inplace)
      (pooling0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1): ReLU(inplace)
      (pooling1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (relu2): ReLU(inplace)
      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3): ReLU(inplace)
      (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu4): ReLU(inplace)
      (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5): ReLU(inplace)
      (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))
      (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu6): ReLU(inplace)
    )
    (rnn): Sequential(
      (0): BidirectionalLSTM(
        (rnn): LSTM(512, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): BidirectionalLSTM(
        (rnn): LSTM(256, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=89, bias=True)
      )
    )
  )
)
Starting training...
The image has shape:
(109, 875)
The image has shape:
(101, 840)
The image has shape:
(103, 1103)
The image has shape:
(109, 1171)
The image has shape:
(146, 602)
The image has shape:
(145, 638)
The image has shape:
(107, 937)
The image has shape:
(83, 925)
The image has shape:
(115, 445)
The image has shape:
(223, 1152)
The image has shape:
(113, 1189)
The image has shape:
(111, 1178)
The image has shape:
(180, 1031)
The image has shape:
(93, 1054)
The image has shape:
(137, 1183)
The image has shape:
(99, 1117)
The image has shape:
(142, 1044)
The image has shape:
(145, 1090)
The image has shape:
(91, 1183)
The image has shape:
(90, 112)
The image has shape:
(81, 292)
The image has shape:
(55, 142)
The image has shape:
(133, 1096)
The image has shape:
(133, 741)
The image has shape:
(179, 1236)
The image has shape:
(90, 1080)
The image has shape:
(94, 518)
The image has shape:
(64, 74)
The image has shape:
(108, 1208)
The image has shape:
(129, 1162)
The image has shape:
(116, 1170)
The image has shape:
(120, 1220)
The image has shape:
(80, 1134)
The image has shape:
(94, 417)
The image has shape:
(124, 1327)
The image has shape:
(113, 1279)
The image has shape:
(146, 1109)
The image has shape:
(122, 999)
The image has shape:
(102, 1118)
The image has shape:
(111, 1014)
The image has shape:
(98, 877)
The image has shape:
(103, 968)
The image has shape:
(183, 624)
The image has shape:
(316, 1525)
The image has shape:
(111, 1083)
The image has shape:
(105, 1087)
The image has shape:
(129, 976)
The image has shape:
(120, 966)
The image has shape:
(137, 1190)
The image has shape:
(139, 1248)
The image has shape:
(139, 1158)
The image has shape:
(131, 1103)
The image has shape:
(137, 1041)
The image has shape:
(110, 964)
The image has shape:
(140, 1127)
The image has shape:
(135, 1160)
The image has shape:
(95, 1154)
The image has shape:
(111, 1139)
The image has shape:
(112, 1073)
The image has shape:
(118, 1019)
The image has shape:
(106, 1270)
The image has shape:
(109, 1173)
The image has shape:
(125, 506)
The image has shape:
(48, 136)
The image has shape:
(253, 1208)
The image has shape:
(133, 1088)
The image has shape:
(121, 1024)
The image has shape:
(110, 1117)
The image has shape:
(123, 1058)
The image has shape:
(135, 1213)
The image has shape:
(119, 366)
The image has shape:
(144, 1120)
The image has shape:
(105, 983)
The image has shape:
(101, 884)
The image has shape:
(101, 194)
The image has shape:
(117, 443)
The image has shape:
(128, 1029)
The image has shape:
(80, 1137)
The image has shape:
(116, 1312)
The image has shape:
(107, 1343)
The image has shape:
(115, 1020)
The image has shape:
(113, 1128)
The image has shape:
(121, 584)
The image has shape:
(137, 1050)
The image has shape:
(111, 873)
The image has shape:
(96, 951)
The image has shape:
(44, 194)
The image has shape:
(121, 438)
The image has shape:
(88, 964)
The image has shape:
(105, 1054)
The image has shape:
(95, 880)
The image has shape:
(99, 990)
The image has shape:
(100, 1132)
The image has shape:
(103, 1147)
The image has shape:
(109, 1028)
The image has shape:
(122, 1055)
The image has shape:
(91, 1108)
The image has shape:
(94, 999)
The image has shape:
(99, 1037)
The image has shape:
(114, 831)
The image has shape:
(130, 1078)
The image has shape:
(123, 1082)
The image has shape:
(116, 1197)
The image has shape:
(124, 1135)
The image has shape:
(108, 966)
The image has shape:
(96, 986)
The image has shape:
(223, 1004)
The image has shape:
(81, 212)
The image has shape:
(86, 128)
The image has shape:
(122, 973)
The image has shape:
(72, 115)
The image has shape:
(65, 122)
The image has shape:
(114, 974)
The image has shape:
(138, 1141)
The image has shape:
(94, 930)
The image has shape:
(119, 1040)
The image has shape:
(115, 1310)
The image has shape:
(99, 1198)
The image has shape:
(109, 1088)
The image has shape:
(102, 1217)
The image has shape:
(112, 1130)
The image has shape:
(103, 1101)
The image has shape:
(134, 1233)
The image has shape:
(105, 1177)
The image has shape:
(83, 978)
The image has shape:
(141, 429)
The image has shape:
(120, 1149)
The image has shape:
(110, 1129)
The image has shape:
(128, 1100)
The image has shape:
(122, 1143)
The image has shape:
(98, 1103)
The image has shape:
(90, 337)
The image has shape:
(105, 1082)
The image has shape:
(102, 1093)
The image has shape:
(208, 1277)
The image has shape:
(86, 1300)
The image has shape:
(199, 1473)
The image has shape:
(120, 1353)
The image has shape:
(98, 1120)
The image has shape:
(110, 1197)
