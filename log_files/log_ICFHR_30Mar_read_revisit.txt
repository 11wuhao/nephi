Running with options: Namespace(adadelta=False, adam=False, alphabet='0123456789abcdefghijklmnopqrstuvwxyz', batchSize=2, beta1=0.5, crnn='experiments/expr_READ_30Mar_revisit/netCRNN_5_4184.pth', cuda=True, displayInterval=120, experiment='experiments/expr_READ_30Mar_revisit', imgH=60, imgW=240, keep_ratio=True, lr=0.0001, n_test_disp=10, ngpu=1, nh=256, niter=200, random_sample=False, saveEpoch=5, test_file='test_file', test_icfhr=False, trainroot='/deep_data/nephi/data/lmdb_read_new/train', valEpoch=5, valroot='/deep_data/nephi/data/lmdb_read_new/val', workers=10)
Random Seed:  2204
This is the alphabet:
40FreihatnūfsRb¬:vdgüomw.DlENBcLkPzAOuVȳGSCIH19Tj62,W5KōZpäÿMJ38x()āÖqß̄—öU7̈ē-Q<>/¾yY+ 
loading pretrained model from experiments/expr_READ_30Mar_revisit/netCRNN_5_4184.pth
Your neural network: DataParallel(
  (module): CRNN(
    (cnn): Sequential(
      (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu0): ReLU(inplace)
      (pooling0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1): ReLU(inplace)
      (pooling1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)
      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (relu2): ReLU(inplace)
      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3): ReLU(inplace)
      (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu4): ReLU(inplace)
      (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5): ReLU(inplace)
      (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=(1, 1), ceil_mode=False)
      (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))
      (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu6): ReLU(inplace)
    )
    (rnn): Sequential(
      (0): BidirectionalLSTM(
        (rnn): LSTM(512, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=256, bias=True)
      )
      (1): BidirectionalLSTM(
        (rnn): LSTM(256, 256, bidirectional=True)
        (embedding): Linear(in_features=512, out_features=89, bias=True)
      )
    )
  )
)
Starting training...
[0/200][120/4184] Loss: 8.239167
[0/200][240/4184] Loss: 5.806197
[0/200][360/4184] Loss: 5.750473
[0/200][480/4184] Loss: 5.613049
[0/200][600/4184] Loss: 5.001962
[0/200][720/4184] Loss: 7.443362
[0/200][840/4184] Loss: 6.850029
[0/200][960/4184] Loss: 6.084599
[0/200][1080/4184] Loss: 5.374811
[0/200][1200/4184] Loss: 5.512505
[0/200][1320/4184] Loss: 5.117526
[0/200][1440/4184] Loss: 5.205746
[0/200][1560/4184] Loss: 8.461566
[0/200][1680/4184] Loss: 8.098151
[0/200][1800/4184] Loss: 5.551548
[0/200][1920/4184] Loss: 4.419066
[0/200][2040/4184] Loss: 5.084791
[0/200][2160/4184] Loss: 4.760991
[0/200][2280/4184] Loss: 6.041699
[0/200][2400/4184] Loss: 5.477557
[0/200][2520/4184] Loss: 4.413881
[0/200][2640/4184] Loss: 5.918489
[0/200][2760/4184] Loss: 4.901506
[0/200][2880/4184] Loss: 5.503461
[0/200][3000/4184] Loss: 5.921842
[0/200][3120/4184] Loss: 5.659186
[0/200][3240/4184] Loss: 4.911455
[0/200][3360/4184] Loss: 6.949241
[0/200][3480/4184] Loss: 5.128867
[0/200][3600/4184] Loss: 5.110997
[0/200][3720/4184] Loss: 4.891584
[0/200][3840/4184] Loss: 4.210621
[0/200][3960/4184] Loss: 4.256725
[0/200][4080/4184] Loss: 5.151690
Start validation set
-                    => v                   , gt: vnnser lieben Fraūen
Total number of images in validation set:     1043
Test loss: 12.594170, accuracy: 0.115900
Character error rate mean: 0.3248; Character error rate sd: 0.9364
Word error rate mean: 0.6738; Word error rate sd: 0.5245
Start validation set
---RR-------------a-------ttt---ss---h----a----------ū-----s------  -e-------rr-----f-----oo-------r-----d-----e----------nn----tt----:-  v-------nn--d----- => Ratshaūs erfordent: vnd, gt: Ratshaūs erfordert: vnd
----------------nn--aa--------cch----g-----s----e----hh--ee-----n-----  ww------------e-------r-----d----ee----n------..   v---------nn-dd------------------ => nachgsehen werden. vnd, gt: nachgsehen werden. vnd
Total number of images in validation set:     2000
Test loss: 6.178143, accuracy: 0.257000
Character error rate mean: 0.1286; Character error rate sd: 0.2914
Word error rate mean: 0.4051; Word error rate sd: 0.4015
Saving epoch experiments/expr_READ_30Mar_revisit/netCRNN_0_4184.pth
[1/200][120/4184] Loss: 4.221175
[1/200][240/4184] Loss: 4.771760
[1/200][360/4184] Loss: 6.210814
[1/200][480/4184] Loss: 5.014038
[1/200][600/4184] Loss: 4.917998
[1/200][720/4184] Loss: 5.217871
[1/200][840/4184] Loss: 4.810983
[1/200][960/4184] Loss: 5.417383
[1/200][1080/4184] Loss: 4.598403
[1/200][1200/4184] Loss: 4.117407
[1/200][1320/4184] Loss: 5.683077
[1/200][1440/4184] Loss: 4.672068
[1/200][1560/4184] Loss: 5.148203
[1/200][1680/4184] Loss: 4.226108
[1/200][1800/4184] Loss: 6.100833
[1/200][1920/4184] Loss: 4.194721
[1/200][2040/4184] Loss: 4.571895
[1/200][2160/4184] Loss: 4.095860
[1/200][2280/4184] Loss: 4.966049
[1/200][2400/4184] Loss: 5.813642
[1/200][2520/4184] Loss: 4.281164
[1/200][2640/4184] Loss: 4.391566
[1/200][2760/4184] Loss: 4.027988
[1/200][2880/4184] Loss: 4.168873
[1/200][3000/4184] Loss: 4.391582
[1/200][3120/4184] Loss: 4.458899
[1/200][3240/4184] Loss: 5.012806
[1/200][3360/4184] Loss: 4.327140
[1/200][3480/4184] Loss: 4.587016
[1/200][3600/4184] Loss: 4.654313
[1/200][3720/4184] Loss: 3.637594
[1/200][3840/4184] Loss: 4.179589
[1/200][3960/4184] Loss: 3.945579
[1/200][4080/4184] Loss: 3.937348
[2/200][120/4184] Loss: 4.081788
[2/200][240/4184] Loss: 3.990595
[2/200][360/4184] Loss: 3.990881
[2/200][480/4184] Loss: 4.655165
[2/200][600/4184] Loss: 3.716285
[2/200][720/4184] Loss: 3.956792
[2/200][840/4184] Loss: 3.816709
[2/200][960/4184] Loss: 4.305161
[2/200][1080/4184] Loss: 4.029773
[2/200][1200/4184] Loss: 3.940580
[2/200][1320/4184] Loss: 3.552633
[2/200][1440/4184] Loss: 3.752854
[2/200][1560/4184] Loss: 5.307498
[2/200][1680/4184] Loss: 7.001255
[2/200][1800/4184] Loss: 3.821116
[2/200][1920/4184] Loss: 3.867420
[2/200][2040/4184] Loss: 4.066877
[2/200][2160/4184] Loss: 3.740490
[2/200][2280/4184] Loss: 3.285811
[2/200][2400/4184] Loss: 3.918094
[2/200][2520/4184] Loss: 5.066402
[2/200][2640/4184] Loss: 3.576917
[2/200][2760/4184] Loss: 4.075863
[2/200][2880/4184] Loss: 4.115495
[2/200][3000/4184] Loss: 3.233043
[2/200][3120/4184] Loss: 4.669730
[2/200][3240/4184] Loss: 4.523377
[2/200][3360/4184] Loss: 3.562401
[2/200][3480/4184] Loss: 3.795589
[2/200][3600/4184] Loss: 4.546287
[2/200][3720/4184] Loss: 3.966649
[2/200][3840/4184] Loss: 3.774739
[2/200][3960/4184] Loss: 3.269893
[2/200][4080/4184] Loss: 4.100464
[3/200][120/4184] Loss: 3.393161
[3/200][240/4184] Loss: 3.941641
[3/200][360/4184] Loss: 3.570018
[3/200][480/4184] Loss: 3.231877
[3/200][600/4184] Loss: 3.747062
[3/200][720/4184] Loss: 3.954095
[3/200][840/4184] Loss: 3.450099
[3/200][960/4184] Loss: 3.373486
[3/200][1080/4184] Loss: 4.183272
[3/200][1200/4184] Loss: 3.445224
[3/200][1320/4184] Loss: 3.230963
[3/200][1440/4184] Loss: 3.278508
[3/200][1560/4184] Loss: 3.339429
[3/200][1680/4184] Loss: 4.618272
[3/200][1800/4184] Loss: 4.002279
[3/200][1920/4184] Loss: 3.448460
[3/200][2040/4184] Loss: 3.490953
[3/200][2160/4184] Loss: 3.750051
[3/200][2280/4184] Loss: 3.370047
[3/200][2400/4184] Loss: 3.584597
[3/200][2520/4184] Loss: 4.177912
[3/200][2640/4184] Loss: 3.417285
[3/200][2760/4184] Loss: 4.350274
[3/200][2880/4184] Loss: 3.303971
[3/200][3000/4184] Loss: 4.981045
[3/200][3120/4184] Loss: 2.770353
[3/200][3240/4184] Loss: 3.084094
[3/200][3360/4184] Loss: 3.511718
[3/200][3480/4184] Loss: 3.520425
[3/200][3600/4184] Loss: 2.832697
[3/200][3720/4184] Loss: 4.004766
[3/200][3840/4184] Loss: 4.122249
[3/200][3960/4184] Loss: 3.154454
[3/200][4080/4184] Loss: 3.503215
[4/200][120/4184] Loss: 3.172082
[4/200][240/4184] Loss: 3.551167
[4/200][360/4184] Loss: 3.097809
[4/200][480/4184] Loss: 3.431671
[4/200][600/4184] Loss: 3.369155
[4/200][720/4184] Loss: 3.479675
[4/200][840/4184] Loss: 3.903325
[4/200][960/4184] Loss: 3.057838
[4/200][1080/4184] Loss: 3.172892
[4/200][1200/4184] Loss: 3.828595
[4/200][1320/4184] Loss: 3.887744
[4/200][1440/4184] Loss: 3.984852
[4/200][1560/4184] Loss: 3.311973
[4/200][1680/4184] Loss: 3.349897
[4/200][1800/4184] Loss: 3.207206
[4/200][1920/4184] Loss: 2.804438
[4/200][2040/4184] Loss: 2.958437
[4/200][2160/4184] Loss: 3.520630
[4/200][2280/4184] Loss: 2.525080
[4/200][2400/4184] Loss: 2.750513
[4/200][2520/4184] Loss: 3.444786
[4/200][2640/4184] Loss: 2.892548
[4/200][2760/4184] Loss: 3.182003
[4/200][2880/4184] Loss: 3.602155
[4/200][3000/4184] Loss: 2.898232
[4/200][3120/4184] Loss: 2.815307
[4/200][3240/4184] Loss: 2.980657
[4/200][3360/4184] Loss: 3.008851
[4/200][3480/4184] Loss: 2.379348
[4/200][3600/4184] Loss: 4.052045
[4/200][3720/4184] Loss: 2.890863
[4/200][3840/4184] Loss: 2.713677
[4/200][3960/4184] Loss: 2.752733
[4/200][4080/4184] Loss: 2.472432
[5/200][120/4184] Loss: 3.258630
[5/200][240/4184] Loss: 3.763344
[5/200][360/4184] Loss: 3.856675
[5/200][480/4184] Loss: 3.148932
[5/200][600/4184] Loss: 3.631111
[5/200][720/4184] Loss: 2.586119
[5/200][840/4184] Loss: 3.603177
[5/200][960/4184] Loss: 3.075263
[5/200][1080/4184] Loss: 2.813614
[5/200][1200/4184] Loss: 3.034270
[5/200][1320/4184] Loss: 3.042868
[5/200][1440/4184] Loss: 3.581138
[5/200][1560/4184] Loss: 2.821490
[5/200][1680/4184] Loss: 2.993431
[5/200][1800/4184] Loss: 3.911208
[5/200][1920/4184] Loss: 4.760906
[5/200][2040/4184] Loss: 3.442503
[5/200][2160/4184] Loss: 2.519485
[5/200][2280/4184] Loss: 2.955354
[5/200][2400/4184] Loss: 2.877644
[5/200][2520/4184] Loss: 3.096869
[5/200][2640/4184] Loss: 2.516632
[5/200][2760/4184] Loss: 2.803363
[5/200][2880/4184] Loss: 2.329274
[5/200][3000/4184] Loss: 3.198623
[5/200][3120/4184] Loss: 2.255105
[5/200][3240/4184] Loss: 2.333486
[5/200][3360/4184] Loss: 2.532569
[5/200][3480/4184] Loss: 2.645040
[5/200][3600/4184] Loss: 2.668168
[5/200][3720/4184] Loss: 2.452756
[5/200][3840/4184] Loss: 3.007635
[5/200][3960/4184] Loss: 2.976049
[5/200][4080/4184] Loss: 2.537843
Start validation set
-                    => L                   , gt: Leibs. vnd Lebensgfahr
Total number of images in validation set:     1043
Test loss: 11.342960, accuracy: 0.172414
Character error rate mean: 0.2311; Character error rate sd: 0.7643
Word error rate mean: 0.5964; Word error rate sd: 0.5278
Start validation set
h----o-------ccchh-----zz---ee----i------tt----:---   --3------6------ k--------..----  vv--------nn---dd----  vv-------o------nn-----  a---------iiin---ee----rr-- => hochzeit: 36 k. vnd von ainer, gt: hochzeit: 36 k. vnd von ainer
-g-----ee----------m----a-------ii----n--ee----n---   H-------o-------cchh-----zz----ee----ii---tt-----:---  22---------4-------- kk---------..-   f-----i---rr---- => gemainen Hochzeit: 24 k. fir, gt: gemainen Hochtzeit: 24 k. fir 
Total number of images in validation set:     2000
Test loss: 3.211557, accuracy: 0.475500
Character error rate mean: 0.0623; Character error rate sd: 0.1618
Word error rate mean: 0.2379; Word error rate sd: 0.3529
Saving epoch experiments/expr_READ_30Mar_revisit/netCRNN_5_4184.pth
[6/200][120/4184] Loss: 2.528954
[6/200][240/4184] Loss: 2.555496
[6/200][360/4184] Loss: 3.311770
[6/200][480/4184] Loss: 2.868122
[6/200][600/4184] Loss: 2.390540
[6/200][720/4184] Loss: 2.932535
[6/200][840/4184] Loss: 2.520876
[6/200][960/4184] Loss: 2.520660
[6/200][1080/4184] Loss: 2.849460
[6/200][1200/4184] Loss: 2.586026
[6/200][1320/4184] Loss: 3.950715
[6/200][1440/4184] Loss: 2.515170
[6/200][1560/4184] Loss: 2.879275
[6/200][1680/4184] Loss: 2.313265
[6/200][1800/4184] Loss: 2.215716
[6/200][1920/4184] Loss: 3.317073
[6/200][2040/4184] Loss: 2.665317
[6/200][2160/4184] Loss: 2.254995
[6/200][2280/4184] Loss: 2.237818
[6/200][2400/4184] Loss: 2.272342
[6/200][2520/4184] Loss: 2.882195
[6/200][2640/4184] Loss: 2.954410
[6/200][2760/4184] Loss: 2.927489
[6/200][2880/4184] Loss: 2.160232
[6/200][3000/4184] Loss: 3.121969
[6/200][3120/4184] Loss: 2.405416
[6/200][3240/4184] Loss: 2.471090
[6/200][3360/4184] Loss: 2.356022
[6/200][3480/4184] Loss: 2.364794
[6/200][3600/4184] Loss: 1.985326
[6/200][3720/4184] Loss: 2.529565
[6/200][3840/4184] Loss: 2.558472
[6/200][3960/4184] Loss: 3.108093
[6/200][4080/4184] Loss: 2.343130
[7/200][120/4184] Loss: 2.467761
[7/200][240/4184] Loss: 2.280042
[7/200][360/4184] Loss: 2.461304
[7/200][480/4184] Loss: 2.547865
[7/200][600/4184] Loss: 2.865339
[7/200][720/4184] Loss: 2.458395
[7/200][840/4184] Loss: 3.050957
[7/200][960/4184] Loss: 2.175648
[7/200][1080/4184] Loss: 2.080095
[7/200][1200/4184] Loss: 2.203711
[7/200][1320/4184] Loss: 2.381155
[7/200][1440/4184] Loss: 3.624068
[7/200][1560/4184] Loss: 3.056365
[7/200][1680/4184] Loss: 2.237775
[7/200][1800/4184] Loss: 2.649623
[7/200][1920/4184] Loss: 2.106118
[7/200][2040/4184] Loss: 2.581491
[7/200][2160/4184] Loss: 2.529324
[7/200][2280/4184] Loss: 1.911953
[7/200][2400/4184] Loss: 2.003582
[7/200][2520/4184] Loss: 2.247565
[7/200][2640/4184] Loss: 2.476658
[7/200][2760/4184] Loss: 2.086977
[7/200][2880/4184] Loss: 2.726715
[7/200][3000/4184] Loss: 2.109682
[7/200][3120/4184] Loss: 1.920473
[7/200][3240/4184] Loss: 2.228809
[7/200][3360/4184] Loss: 1.725582
[7/200][3480/4184] Loss: 1.981059
[7/200][3600/4184] Loss: 2.139227
[7/200][3720/4184] Loss: 2.381991
[7/200][3840/4184] Loss: 2.138264
[7/200][3960/4184] Loss: 2.038988
[7/200][4080/4184] Loss: 1.851824
[8/200][120/4184] Loss: 2.262468
[8/200][240/4184] Loss: 2.568666
[8/200][360/4184] Loss: 1.683288
[8/200][480/4184] Loss: 2.275318
[8/200][600/4184] Loss: 1.982524
[8/200][720/4184] Loss: 2.076608
[8/200][840/4184] Loss: 2.227538
[8/200][960/4184] Loss: 2.066565
[8/200][1080/4184] Loss: 2.056321
[8/200][1200/4184] Loss: 3.260668
[8/200][1320/4184] Loss: 2.207334
[8/200][1440/4184] Loss: 2.340310
[8/200][1560/4184] Loss: 1.786603
[8/200][1680/4184] Loss: 2.556298
[8/200][1800/4184] Loss: 2.106463
[8/200][1920/4184] Loss: 2.141180
[8/200][2040/4184] Loss: 1.664555
[8/200][2160/4184] Loss: 1.933666
[8/200][2280/4184] Loss: 1.906383
[8/200][2400/4184] Loss: 2.370548
[8/200][2520/4184] Loss: 2.000188
[8/200][2640/4184] Loss: 1.923496
[8/200][2760/4184] Loss: 2.385481
[8/200][2880/4184] Loss: 3.214148
[8/200][3000/4184] Loss: 1.872255
[8/200][3120/4184] Loss: 1.812382
[8/200][3240/4184] Loss: 1.992856
[8/200][3360/4184] Loss: 1.949629
[8/200][3480/4184] Loss: 2.169893
[8/200][3600/4184] Loss: 1.952317
[8/200][3720/4184] Loss: 2.157456
[8/200][3840/4184] Loss: 1.795351
[8/200][3960/4184] Loss: 1.647061
[8/200][4080/4184] Loss: 1.612835
[9/200][120/4184] Loss: 1.934634
[9/200][240/4184] Loss: 1.732502
[9/200][360/4184] Loss: 1.907904
[9/200][480/4184] Loss: 2.298186
[9/200][600/4184] Loss: 2.141584
[9/200][720/4184] Loss: 1.677449
[9/200][840/4184] Loss: 2.496168
[9/200][960/4184] Loss: 1.972751
[9/200][1080/4184] Loss: 1.861555
[9/200][1200/4184] Loss: 2.139038
[9/200][1320/4184] Loss: 2.053302
[9/200][1440/4184] Loss: 1.579209
[9/200][1560/4184] Loss: 2.114912
[9/200][1680/4184] Loss: 1.823226
[9/200][1800/4184] Loss: 2.036340
[9/200][1920/4184] Loss: 1.518364
[9/200][2040/4184] Loss: 1.759045
[9/200][2160/4184] Loss: 1.772171
[9/200][2280/4184] Loss: 1.636439
[9/200][2400/4184] Loss: 1.968220
[9/200][2520/4184] Loss: 2.083756
[9/200][2640/4184] Loss: 1.932427
[9/200][2760/4184] Loss: 1.860005
[9/200][2880/4184] Loss: 1.701682
[9/200][3000/4184] Loss: 2.250137
[9/200][3120/4184] Loss: 2.477092
[9/200][3240/4184] Loss: 2.127372
[9/200][3360/4184] Loss: 1.503956
[9/200][3480/4184] Loss: 1.738128
[9/200][3600/4184] Loss: 1.930293
[9/200][3720/4184] Loss: 1.885953
[9/200][3840/4184] Loss: 2.595851
[9/200][3960/4184] Loss: 1.687821
[9/200][4080/4184] Loss: 2.296893
[10/200][120/4184] Loss: 2.291973
[10/200][240/4184] Loss: 1.776555
[10/200][360/4184] Loss: 1.626660
[10/200][480/4184] Loss: 1.786528
[10/200][600/4184] Loss: 1.350311
[10/200][720/4184] Loss: 1.874371
[10/200][840/4184] Loss: 1.821409
[10/200][960/4184] Loss: 2.353465
[10/200][1080/4184] Loss: 1.463128
[10/200][1200/4184] Loss: 1.996085
[10/200][1320/4184] Loss: 1.569115
[10/200][1440/4184] Loss: 1.792914
[10/200][1560/4184] Loss: 1.862470
[10/200][1680/4184] Loss: 1.391421
[10/200][1800/4184] Loss: 2.253414
[10/200][1920/4184] Loss: 1.640302
[10/200][2040/4184] Loss: 1.457904
[10/200][2160/4184] Loss: 1.637404
[10/200][2280/4184] Loss: 1.751339
[10/200][2400/4184] Loss: 1.516424
[10/200][2520/4184] Loss: 1.748823
[10/200][2640/4184] Loss: 1.625708
[10/200][2760/4184] Loss: 2.064791
[10/200][2880/4184] Loss: 1.486791
[10/200][3000/4184] Loss: 1.891822
[10/200][3120/4184] Loss: 1.679483
[10/200][3240/4184] Loss: 1.368491
[10/200][3360/4184] Loss: 1.864096
[10/200][3480/4184] Loss: 1.557009
[10/200][3600/4184] Loss: 1.711915
[10/200][3720/4184] Loss: 1.655745
[10/200][3840/4184] Loss: 1.713495
[10/200][3960/4184] Loss: 1.802312
[10/200][4080/4184] Loss: 1.467260
Start validation set
-                    => h                   , gt: hat dabei Zūverbleiben.
Total number of images in validation set:     1043
Test loss: 12.719478, accuracy: 0.179119
Character error rate mean: 0.2587; Character error rate sd: 0.8885
Word error rate mean: 0.5295; Word error rate sd: 0.4489
Start validation set
SS----ccchh--g--------rr-----a-------ff----e-----rr---..---  v-------nn---dd--------   W------------- -l--------h---a-------l---------m-------    G---------------o-------ll-----d-------¬ => Schgrafer. vnd W lhalm Gold¬, gt: Schgrafer. vnd Wilhalm Gold¬
-----------w------------------ū---r---------m----   S-----tt----a------t-----tt----::-   v-----n---dd-----   RR------------a--------tt---ss--ccchh--r-----e---ii----bb---e-----rr--------- => wūrm Statt: vnd Ratschreiber, gt: wūrm Statt: vnd Ratschreiber
Total number of images in validation set:     2000
Test loss: 2.296497, accuracy: 0.559500
Character error rate mean: 0.0481; Character error rate sd: 0.1720
Word error rate mean: 0.1927; Word error rate sd: 0.3055
Saving epoch experiments/expr_READ_30Mar_revisit/netCRNN_10_4184.pth
[11/200][120/4184] Loss: 1.666751
[11/200][240/4184] Loss: 1.562276
[11/200][360/4184] Loss: 1.785893
[11/200][480/4184] Loss: 1.652006
[11/200][600/4184] Loss: 1.965654
[11/200][720/4184] Loss: 1.439108
[11/200][840/4184] Loss: 1.641354
[11/200][960/4184] Loss: 1.518528
[11/200][1080/4184] Loss: 1.926833
[11/200][1200/4184] Loss: 1.402631
[11/200][1320/4184] Loss: 2.060390
[11/200][1440/4184] Loss: 1.660376
[11/200][1560/4184] Loss: 1.417652
[11/200][1680/4184] Loss: 1.623121
[11/200][1800/4184] Loss: 1.598751
[11/200][1920/4184] Loss: 1.231320
[11/200][2040/4184] Loss: 1.488527
[11/200][2160/4184] Loss: 2.568165
[11/200][2280/4184] Loss: 2.280230
[11/200][2400/4184] Loss: 1.329641
[11/200][2520/4184] Loss: 1.395318
[11/200][2640/4184] Loss: 1.443324
[11/200][2760/4184] Loss: 1.358571
[11/200][2880/4184] Loss: 1.158200
[11/200][3000/4184] Loss: 1.708841
[11/200][3120/4184] Loss: 1.617759
[11/200][3240/4184] Loss: 1.768647
[11/200][3360/4184] Loss: 2.404739
[11/200][3480/4184] Loss: 1.920225
[11/200][3600/4184] Loss: 1.347577
[11/200][3720/4184] Loss: 1.589100
[11/200][3840/4184] Loss: 1.615230
[11/200][3960/4184] Loss: 1.178865
[11/200][4080/4184] Loss: 2.583975
[12/200][120/4184] Loss: 1.352416
[12/200][240/4184] Loss: nan
[12/200][360/4184] Loss: 1.495853
[12/200][480/4184] Loss: 1.411764
[12/200][600/4184] Loss: 1.217665
[12/200][720/4184] Loss: 1.245655
[12/200][840/4184] Loss: 1.404580
[12/200][960/4184] Loss: 1.569305
[12/200][1080/4184] Loss: 1.006787
[12/200][1200/4184] Loss: 1.330813
[12/200][1320/4184] Loss: 1.249650
[12/200][1440/4184] Loss: 1.444326
[12/200][1560/4184] Loss: 1.748146
[12/200][1680/4184] Loss: 1.437060
[12/200][1800/4184] Loss: 1.255087
[12/200][1920/4184] Loss: 2.786801
[12/200][2040/4184] Loss: 1.621172
[12/200][2160/4184] Loss: 0.934237
[12/200][2280/4184] Loss: 1.414889
[12/200][2400/4184] Loss: 1.488409
[12/200][2520/4184] Loss: 1.386515
[12/200][2640/4184] Loss: 1.423142
[12/200][2760/4184] Loss: 1.822320
[12/200][2880/4184] Loss: 1.614799
[12/200][3000/4184] Loss: 1.360894
[12/200][3120/4184] Loss: 1.314799
[12/200][3240/4184] Loss: 1.304618
[12/200][3360/4184] Loss: 1.343985
[12/200][3480/4184] Loss: 1.686350
[12/200][3600/4184] Loss: 1.143915
[12/200][3720/4184] Loss: 1.353832
[12/200][3840/4184] Loss: 1.279893
[12/200][3960/4184] Loss: 1.288484
[12/200][4080/4184] Loss: 1.144941
[13/200][120/4184] Loss: 1.192292
[13/200][240/4184] Loss: 1.293666
[13/200][360/4184] Loss: 1.375828
[13/200][480/4184] Loss: 1.360622
[13/200][600/4184] Loss: 1.415395
[13/200][720/4184] Loss: 1.565718
[13/200][840/4184] Loss: 1.648233
[13/200][960/4184] Loss: 0.959407
[13/200][1080/4184] Loss: 1.797180
[13/200][1200/4184] Loss: 1.519260
[13/200][1320/4184] Loss: 1.106288
[13/200][1440/4184] Loss: 1.735643
[13/200][1560/4184] Loss: 1.116096
[13/200][1680/4184] Loss: 0.982934
[13/200][1800/4184] Loss: 1.233534
[13/200][1920/4184] Loss: 1.414341
[13/200][2040/4184] Loss: 1.576624
[13/200][2160/4184] Loss: 1.250714
[13/200][2280/4184] Loss: 1.398906
[13/200][2400/4184] Loss: 1.340523
[13/200][2520/4184] Loss: 1.235894
[13/200][2640/4184] Loss: 1.757150
[13/200][2760/4184] Loss: 1.164597
[13/200][2880/4184] Loss: 1.095377
[13/200][3000/4184] Loss: 1.421602
[13/200][3120/4184] Loss: 1.395536
[13/200][3240/4184] Loss: 1.458278
[13/200][3360/4184] Loss: 1.044219
[13/200][3480/4184] Loss: 1.140712
[13/200][3600/4184] Loss: 1.154011
[13/200][3720/4184] Loss: 1.081508
[13/200][3840/4184] Loss: 1.362409
[13/200][3960/4184] Loss: 2.085488
[13/200][4080/4184] Loss: 1.177811
[14/200][120/4184] Loss: 1.271471
[14/200][240/4184] Loss: 1.383081
[14/200][360/4184] Loss: 1.356051
[14/200][480/4184] Loss: 2.237855
[14/200][600/4184] Loss: 1.132959
[14/200][720/4184] Loss: 1.475392
[14/200][840/4184] Loss: 1.211166
[14/200][960/4184] Loss: 1.224164
[14/200][1080/4184] Loss: 1.087580
[14/200][1200/4184] Loss: 1.807937
[14/200][1320/4184] Loss: 1.126603
[14/200][1440/4184] Loss: 1.734250
[14/200][1560/4184] Loss: 1.281598
[14/200][1680/4184] Loss: 1.041430
[14/200][1800/4184] Loss: 0.902704
[14/200][1920/4184] Loss: 1.017305
[14/200][2040/4184] Loss: 1.136528
[14/200][2160/4184] Loss: 1.167535
[14/200][2280/4184] Loss: 1.363141
[14/200][2400/4184] Loss: 1.153823
[14/200][2520/4184] Loss: 0.988665
[14/200][2640/4184] Loss: 1.110803
[14/200][2760/4184] Loss: 1.180657
[14/200][2880/4184] Loss: 1.233500
[14/200][3000/4184] Loss: 1.095362
[14/200][3120/4184] Loss: 1.050937
