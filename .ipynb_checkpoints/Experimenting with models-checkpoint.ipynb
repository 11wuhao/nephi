{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Russell Ault\n",
    "\n",
    "# This Notebook Contains code for exploring the functionality of the Pytorch Handwriting Recognition Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is clear to me that I need to write interactive code to load a model that we have trained and test it on a validation set, and have it output accuracy and word error rates. \n",
    "\n",
    "## The present way that the library is written is clearly not easily conducive to this\n",
    "\n",
    "## Here are some comments I have about the code and how to improve it:\n",
    "- In evaluating model accuracy a character by character accuracy is being used, not an edit distance. I need to put character and word error rates into the model. I should include a mean and sd of these parameters.\n",
    "- I think I need to just run the validation code right now to see what it does.\n",
    "- I think that the main python module should be refactored to allow its use in other python modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Main Functionality in Notebook fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import os\n",
    "import utils\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models.crnn as crnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin-1\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "stdout = sys.stdout\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('latin-1')\n",
    "from model_error import cer, wer\n",
    "\n",
    "\n",
    "#My workaround was that at the top of the script, I import sys, and store sys.stdout in a separate variable, e.g. stdout.\n",
    "sys.stdout = stdout\n",
    "print(sys.getdefaultencoding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model_error import cer, wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(3+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--trainroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--valroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "parser.add_argument('--imgH', type=int, default=32, help='the height of the input image to network')\n",
    "parser.add_argument('--imgW', type=int, default=100, help='the width of the input image to network')\n",
    "parser.add_argument('--nh', type=int, default=256, help='size of the lstm hidden state')\n",
    "parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='learning rate for Critic, default=0.00005')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--crnn', default='', help=\"path to crnn (to continue training)\")\n",
    "parser.add_argument('--alphabet', type=str, default='0123456789abcdefghijklmnopqrstuvwxyz')\n",
    "parser.add_argument('--experiment', default=None, help='Where to store samples and models')\n",
    "parser.add_argument('--displayInterval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--n_test_disp', type=int, default=10, help='Number of samples to display when test')\n",
    "parser.add_argument('--valInterval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--saveInterval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--adam', action='store_true', help='Whether to use adam (default is rmsprop)')\n",
    "parser.add_argument('--adadelta', action='store_true', help='Whether to use adadelta (default is rmsprop)')\n",
    "parser.add_argument('--keep_ratio', action='store_true', help='whether to keep ratio for image resize')\n",
    "parser.add_argument('--random_sample', action='store_true', help='whether to sample the dataset with random sampler')\n",
    "opt = parser.parse_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if experiment is None:\n",
    "    experiment = 'expr'\n",
    "os.system('mkdir {0}'.format(experiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainroot = \"/home/ubuntu/russell/nephi/data/lmdb/train\"\n",
    "valroot = \"/home/ubuntu/russell/nephi/data/lmdb/val\"\n",
    "batchSize = 64\n",
    "nh = 256                  # size of the LSTM hidden state\n",
    "imgW = 100\n",
    "imgH = 32\n",
    "ngpu = 1\n",
    "beta1 = 0.5\n",
    "lr = 0.0001\n",
    "workers = 10\n",
    "keep_ratio = True\n",
    "adam = True\n",
    "adadelta = False\n",
    "n_test_disp = 100\n",
    "\n",
    "alph_file_dylan = \"/home/ubuntu/dylan/nephi/alphabet.txt\"\n",
    "alph_file_russell = \"/home/ubuntu/russell/nephi/alphabet.txt\"\n",
    "alphabet = '0123456789abcdefghijklmnopqrstuvwxyzB- EÂ¬Ã¼.RSÅ«J/DHA:K¤¿ZLGFNTPCOVWIM<8d>Ä<81><9f>,<93>È³¶'\n",
    "#0123456789abcdefghijklmnopqrstuvwxyzW VCGū¬.HM,ILAZ:BTÿSER<BC>JFāP<9F>NDKOȳ<B6>\n",
    "#<A4><8D>()—̈-<84><93>Q<96>/Y<BE>U<>+  # This is what I got from Dylan's file\n",
    "\n",
    "untrained_crnn_dylan = \"/home/ubuntu/dylan/nephi/expr/netCRNN_1_100.pth\"\n",
    "lesstrained_crnn_dylan = \"/home/ubuntu/dylan/nephi/expr/netCRNN_1000_100.pth\"\n",
    "trained_crnn_russell = \"/home/ubuntu/russell/nephi/expr/netCRNN_3870_100.pth\"\n",
    "trained_crnn_dylan = \"/home/ubuntu/dylan/nephi/expr/netCRNN_3210_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  6501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa9d0bf2ba0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "cuda = True\n",
    "\n",
    "#if torch.cuda.is_available() and not cuda:\n",
    "#    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "train_dataset = dataset.lmdbDataset(root=trainroot)\n",
    "sampler = dataset.randomSequentialSampler(train_dataset, batchSize)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchSize, sampler=sampler,\n",
    "    num_workers=int(workers),\n",
    "    collate_fn=dataset.alignCollate(imgH=imgH, imgW=imgW, keep_ratio=keep_ratio))\n",
    "test_dataset = dataset.lmdbDataset(\n",
    "    root=valroot, transform=dataset.resizeNormalize((imgW, imgH)))   # I have changed this line from the original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the two alphabets\n",
    "alphabet_russell = ''\n",
    "alphabet_dylan = ''\n",
    "\n",
    "with open(alph_file_russell, 'r') as myfile:\n",
    "    alphabet_russell = myfile.read()\n",
    "with open(alph_file_dylan, 'r') as myfile:\n",
    "    alphabet_dylan = myfile.read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the dylan ALphabet and model first\n",
    "alphabet = alphabet_dylan\n",
    "\n",
    "nclass = len(alphabet) + 1\n",
    "nc = 1\n",
    "\n",
    "converter = utils.strLabelConverter(alphabet)\n",
    "criterion = CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on crnn\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crnn = crnn.CRNN(imgH, nc, nclass, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN (\n",
       "  (cnn): Sequential (\n",
       "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu0): ReLU (inplace)\n",
       "    (pooling0): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU (inplace)\n",
       "    (pooling1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (relu2): ReLU (inplace)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU (inplace)\n",
       "    (pooling2): MaxPool2d (size=(2, 2), stride=(2, 1), dilation=(1, 1))\n",
       "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (relu4): ReLU (inplace)\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu5): ReLU (inplace)\n",
       "    (pooling3): MaxPool2d (size=(2, 2), stride=(2, 1), dilation=(1, 1))\n",
       "    (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (relu6): ReLU (inplace)\n",
       "  )\n",
       "  (rnn): Sequential (\n",
       "    (0): BidirectionalLSTM (\n",
       "      (rnn): LSTM(512, 256, bidirectional=True)\n",
       "      (embedding): Linear (512 -> 256)\n",
       "    )\n",
       "    (1): BidirectionalLSTM (\n",
       "      (rnn): LSTM(256, 256, bidirectional=True)\n",
       "      (embedding): Linear (512 -> 97)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crnn.apply(weights_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the above unexpected key error, I will assume that when I try to run the original code with a validation epoch number, I will get the same error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "text = torch.IntTensor(batchSize * 5)          # RA: I don't understand why the text has this size\n",
    "length = torch.IntTensor(batchSize)\n",
    "\n",
    "if cuda:\n",
    "    crnn.cuda()\n",
    "    crnn = torch.nn.DataParallel(crnn, device_ids=range(ngpu))\n",
    "    image = image.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model from /home/ubuntu/dylan/nephi/expr/netCRNN_3210_100.pth\n",
      "DataParallel (\n",
      "  (module): CRNN (\n",
      "    (cnn): Sequential (\n",
      "      (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu0): ReLU (inplace)\n",
      "      (pooling0): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu1): ReLU (inplace)\n",
      "      (pooling1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "      (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu2): ReLU (inplace)\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu3): ReLU (inplace)\n",
      "      (pooling2): MaxPool2d (size=(2, 2), stride=(2, 1), dilation=(1, 1))\n",
      "      (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu4): ReLU (inplace)\n",
      "      (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu5): ReLU (inplace)\n",
      "      (pooling3): MaxPool2d (size=(2, 2), stride=(2, 1), dilation=(1, 1))\n",
      "      (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
      "      (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu6): ReLU (inplace)\n",
      "    )\n",
      "    (rnn): Sequential (\n",
      "      (0): BidirectionalLSTM (\n",
      "        (rnn): LSTM(512, 256, bidirectional=True)\n",
      "        (embedding): Linear (512 -> 256)\n",
      "      )\n",
      "      (1): BidirectionalLSTM (\n",
      "        (rnn): LSTM(256, 256, bidirectional=True)\n",
      "        (embedding): Linear (512 -> 97)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load Dylan's pretrained model first\n",
    "trained_crnn = trained_crnn_dylan\n",
    "if trained_crnn != '':\n",
    "    print('loading pretrained model from %s' % trained_crnn)\n",
    "    crnn.load_state_dict(torch.load(trained_crnn))\n",
    "print(crnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = Variable(image)\n",
    "text = Variable(text)\n",
    "length = Variable(length)\n",
    "\n",
    "# loss averager\n",
    "loss_avg = utils.averager()\n",
    "\n",
    "# setup optimizer\n",
    "if adam:\n",
    "    optimizer = optim.Adam(crnn.parameters(), lr=lr,\n",
    "                           betas=(beta1, 0.999))\n",
    "elif adadelta:\n",
    "    optimizer = optim.Adadelta(crnn.parameters(), lr=lr)\n",
    "else:\n",
    "    optimizer = optim.RMSprop(crnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is where I will test out the code.\n",
    "\n",
    "### First order of business is to see what val outputs currently on these pretrained models using the test set.\n",
    "### Then add word and character error rate and a way to calculate mean and standard deviation of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val(net, dataset, criterion, max_iter=100):\n",
    "    print('Start val')\n",
    "\n",
    "    for p in crnn.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    net.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, shuffle=True, batch_size=batchSize, num_workers=int(workers))\n",
    "    val_iter = iter(data_loader)\n",
    "\n",
    "    i = 0\n",
    "    n_correct = 0\n",
    "    loss_avg = utils.averager()\n",
    "    \n",
    "    image_count = 0\n",
    "    \n",
    "    # Character and word error rate lists\n",
    "    char_error = []\n",
    "    w_error = []\n",
    "\n",
    "    max_iter = min(max_iter, len(data_loader))\n",
    "    #max_iter = len(data_loader)\n",
    "    for i in range(max_iter):\n",
    "        data = val_iter.next()\n",
    "        i += 1\n",
    "        cpu_images, cpu_texts = data\n",
    "        batch_size = cpu_images.size(0)\n",
    "        image_count = image_count + batch_size\n",
    "        utils.loadData(image, cpu_images)\n",
    "        t, l = converter.encode(cpu_texts)\n",
    "        utils.loadData(text, t)\n",
    "        utils.loadData(length, l)\n",
    "\n",
    "        preds = crnn(image)\n",
    "        preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "        cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "        loss_avg.add(cost)\n",
    "        \n",
    "        \n",
    "        # RA: While I am not sure yet, it looks like a greedy decoder and not beam search is being used here\n",
    "        # Also, a simple character by character accuracy is being used, not an edit distance.\n",
    "        # Case is ignored in the accuracy, which is not ideal for an actual working system\n",
    "        \n",
    "        _, preds = preds.max(2)\n",
    "        preds = preds.squeeze(2)\n",
    "        preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "        for pred, target in zip(sim_preds, cpu_texts):\n",
    "            if pred == target.lower():\n",
    "                n_correct += 1\n",
    "            #print(pred)\n",
    "            #print(\"Pred: %s; target: %s\" % (pred, target))\n",
    "            char_error.append(cer(pred, target.lower()))\n",
    "            w_error.append(wer(pred, target.lower()))\n",
    "\n",
    "    raw_preds = converter.decode(preds.data, preds_size.data, raw=True)[:n_test_disp]\n",
    "    for raw_pred, pred, gt in zip(raw_preds, sim_preds, cpu_texts):\n",
    "        print('%-20s => %-20s, gt: %-20s' % (raw_pred, pred, gt))\n",
    "\n",
    "    accuracy = n_correct / float(max_iter * batchSize)\n",
    "    print('Test loss: %f, accuray: %f' % (loss_avg.val(), accuracy))\n",
    "    \n",
    "    char_arr =np.array(char_error)\n",
    "    w_arr = np.array(w_error)\n",
    "    #numpy.std(arr, ddof=1)\n",
    "    #numpy.mean(arr, axis=0)\n",
    "    #print(\"All character error rates:\")\n",
    "    #print(char_error)\n",
    "    #print(\"All word error rates\")\n",
    "    #print(w_error)\n",
    "    print(\"Character error rate mean: %4.4f; Character error rate sd: %4.4f\" % (np.mean(char_arr), np.std(char_arr, ddof=1)))\n",
    "    print(\"Word error rate mean: %4.4f; Word error rate sd: %4.4f\" % (np.mean(w_arr), np.std(w_arr, ddof=1)))\n",
    "    print(\"Total number of images in validation set: %8d\" % image_count)\n",
    "    return (char_error, w_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainBatch(net, criterion, optimizer):\n",
    "    data = train_iter.next()\n",
    "    cpu_images, cpu_texts = data\n",
    "    batch_size = cpu_images.size(0)\n",
    "    utils.loadData(image, cpu_images)\n",
    "    t, l = converter.encode(cpu_texts)\n",
    "    utils.loadData(text, t)\n",
    "    utils.loadData(length, l)\n",
    "\n",
    "    preds = crnn(image)\n",
    "    preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "    cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "    crnn.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now see how the pre-trained model works on the validation set\n",
    "oops, I have to figure out how to change the kernal of this python notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start val\n",
      "aas dernnaet  ann h- daaa. => as dernaet an h da. , gt: Die Genneral ambts Rait¬\n",
      "llchr t--n dz�-�-n-ewiils. => lchr tn dzÿnewils. , gt: Alhie. sonnder Meniglich\n",
      "aamme-  bllrz--neem-mma-ns => ame blrznemmans     , gt: Ainer Loblichen Regierūng.\n",
      "maa---nin. deoornū�nnssen => manin. deornūnsen  , gt: Locheman. Jeronimūsen\n",
      "wal------n  win---dde----. => waln winde.         , gt: Melchior Wūrmbrandt.\n",
      "air-ranster  aibe -geegmn. => airranster aibe gegmn., gt: aūf negsten Ratstag wid¬\n",
      "aeeaan d2  lea---w- olsien => aean d2 leaw olsien , gt: Aber Ain CoPȳ an Ir G:\n",
      "aab  mal  aenn iimee  d��. => ab mal aen ime d��. , gt: Als well man seiner aūf\n",
      "haarautr  dewezzi wed ww-n => harautr dewezi wed wn, gt: Mandaten. dergleichen Wöhrn\n",
      "vme saūrwednt a�h bem-mon => vme saūrwednt a�h bemmon, gt: von Rovereid, aūf Laȳen\n",
      "zei-----merw---ppaa-n---n. => zeimerwpann.        , gt: Reinhardt von Pūechhaim.\n",
      "vnn---dd-ll-tt hze---t---. => vndlt hzet.         , gt: Landts bet:         \n",
      "ma�-�rr  ha--reirgge-nden  => maūr hareirgenden  , gt: Geōrg Schaler liegenden\n",
      "aaa-�mer dvvnd hūn--nnenn => a�mer dvnd hūnnen  , gt: Haimen. vnd Hannsen \n",
      "vnnddziligeisjen lichnfn¬ => vndziligeisjen lichnfn¬, gt: Wilhalmen von KiePach.\n",
      "v--cchhūgnnnn hh�be mmar. => vchūgn h�be mar.   , gt: Wilhalm von Hofkhirchen\n",
      "toa----�ū�--�zzz--laa---h => toaū�zlah          , gt: wonūng. Tax        \n",
      "megg-n�ū  degenden slais. => megnū degenden slais., gt: mag. Zū dienern hat.\n",
      "a�bbl-it  o-  wnwiillaū�t => a�blit o wnwilaūt  , gt: mall. 6 fl gewilligt. \n",
      "Test loss: 134.090870, accuray: 0.000919\n"
     ]
    }
   ],
   "source": [
    "val(crnn, test_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start val\n",
      "imns sggtel�in  wer�n��ntt => imns sgtel�in wer�n�nt, gt: dise Motiūen, worūmb\n",
      "herrren lann-nddshh�ūbba� => heren lanndshūba�  , gt: herrn Lanndthaūbt¬\n",
      "im  rr-s---r  anzzail zint => im rsr anzail zint  , gt: in grosser Anzal aūf.\n",
      "aiiee, a�s  bacchen olllig => aie, a�s bachen olig, gt: Ainer aūf Laÿen etlich \n",
      "die  herr--megehenns. vvnd => die hermegehens. vnd, gt: die Thor angehengt: vnd\n",
      "------------1-----------35 => 135                 , gt: 203                 \n",
      "man--en,, aaddlli- vnnddet => manen, adli vndet   , gt: gannzer Adelich. vnd Er¬\n",
      "vnd ma�-s ee dammedthherr. => vnd ma�s e damedther., gt: vnd Aūsser Lanndtsūer¬\n",
      "vmdd aamz-eiengg-n-e malll => vmd amzeiengne mal  , gt: vnd Aūsgeben Zūūerhalt\n",
      "herrr-n an-nnd h�ūlbta-nn => hern annd hūlbtan  , gt: herrn Lanndthaūbtman\n",
      "zūeggihibenn werdden  iin => zūegihiben werden in, gt: Zūegschriben werden. ain\n",
      "lem--ngaarr  an--ee. bea¬ => lemngar ane. bea¬  , gt: Pennzinger. hannsen Loche¬\n",
      "būeehen  aūnnddn  wofffr => būehen aūndn wofr , gt: būecher, Daūiden Waff¬\n",
      "zam-x.  ddb  raafhhee  tis => zamx. db rafhe tis  , gt: Schmalz. das Roch P: 12 k. 1 f.\n",
      "za----------hh----------�  => zah�                , gt: Caspar              \n",
      "ferrssen-nn vnn parbrmaej. => fersenn vn parbrmaej., gt: Personen one Patenten\n",
      "vdd beeghhe-sen billhsionn => vd beghesen bilhsion, gt: vnd Mathiasen Feichtner\n",
      "pan-- der  a�--ntewwendatt => pan der a�ntewendat , gt: Wann der Grūsstner vmb\n",
      "vo----ns ch------stteeeer. => vons chster.        , gt: Otto Fridrich Geȳer.\n",
      "Test loss: 149.597148, accuray: 0.000919\n"
     ]
    }
   ],
   "source": [
    "val(crnn_dylan_trained, test_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start val\n",
      "-------11---------------66 => 16                  , gt: 193                 \n",
      "gns pūrrrennninssee-  nnd => gns pūreninse nd   , gt: N. Būrgermaisster: vnd\n",
      "haūnddroeeen  srrffffer�. => haūndroen srfer�.  , gt: Anndreen Rotten¬   \n",
      "hannnnnn aūgeen senndeer. => han aūgen sender.  , gt: khomen migen. sonnder\n",
      "m-ttrt hū� hehers tr bi¬ => mtrt hū hehers tr bi¬, gt: Matheūs Hofsteter  \n",
      "airttei wlls s-felwmmmee-s => airtei wls sfelwmes , gt: Aūfkhaūfft. Als soll deßweg\n",
      "pen ssgeegch aa-sccileienn => pen sgegch ascileien, gt: Passbrief Angehalten.\n",
      "fen-ns nnnden ii tetrge--. => fenns nden i tetrge., gt: Personen An Jezt verseh.\n",
      "wee-nn-ntenn vnnndderrlcch => wennten vnderlch    , gt: darūnter sonnderlich\n",
      "han�stt vdd i  grbgggzai¬ => han�st vd i grbgzai¬, gt: haūß. vnd Zū Rūgg Zieh.\n",
      "vn--n-tttgeern----snmfffff => vnntgernsnmf        , gt: vnd versich¬       \n",
      "lileerpos gefeoo wa--f-nr� => lilerpos gefeo wafnr�, gt: es bei der getanen Abschaff¬\n",
      "tatterannichee. gen-ndict  => tateraniche. genndict , gt: Schrannckhen gemacht¬\n",
      "ger-nnweeen ii ten garab-. => gernwen i ten garab., gt: Derowegen sei Ir Gnaden\n",
      "mensst powweee sesss i-e-. => menst powe ses ie.  , gt: wolt das L. P .15 k geb.\n",
      "-------11---------------66 => 16                  , gt: 193                 \n",
      "aan-ger enbeebeen zerroon. => anger enbeben zeron., gt: Inntercediern herrn \n",
      "senn airrrnnnn brmsl ggi�. => sen airn brmsl gi�. , gt: Vom Ingram daselbst \n",
      "aa--spūs  asscz�sbmaaese. => aspūs ascz�sbmaese., gt: gwalts mit haūs wesen\n",
      "Test loss: 139.930357, accuray: 0.000919\n",
      "Character error rate mean: 0.8858; Character error rate sd: 0.4875\n",
      "Word error rate mean: 1.3256; Word error rate sd: 0.6271\n",
      "Total number of images in validation set:     1043\n"
     ]
    }
   ],
   "source": [
    "# Now with the character and word error rates\n",
    "char_e, w_e = val(crnn, test_dataset, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crnn.load_state_dict(torch.load(lesstrained_crnn_dylan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start val\n",
      "ha-  der herrrrūen-nnicht => ha der herūennicht , gt: hat der herr Būrgermaist\n",
      "an� gbllss--nneiierd- ciss => an� gblsneierd cis  , gt: Aūsglassen wūrde. sich \n",
      "gennnnddeennn scshgmm-lh-t => genden scshgmlht    , gt: geennden .2. Vieh Märckht\n",
      "o----lllccher go--n-e-err� => olcher goneer�      , gt: Loblichen Cammer    \n",
      "gr----tt-----r  ete-----¬ => grtr ete¬          , gt: Antoni Jacob        \n",
      "pa---fer peerweennses dder => pafer perwenses der , gt: Talfer Prūggen, so der\n",
      "sa-ss-ss  wesssseiigenmen. => sass weseigenmen.   , gt: Straff verboten wirdet,\n",
      "-------------------------. => .                   , gt: bet:                \n",
      "mi---. va�fth za-tmnnrinj. => mi. va�fth zatmnrinj., gt: groß noch khlain vieh\n",
      "wa--rdden  an- sssllbee-nn => warden an slben     , gt: werden. Inen Zollern\n",
      "ge----see--mpe--s-fer-ier. => gesempesferier.     , gt: Caspar Artsteter.   \n",
      "vn---ndd  paa-----n----tr. => vnnd pantr.         , gt: vnd Lanndts¬       \n",
      "v-------iibb----------t--. => vibt.               , gt: willigt.            \n",
      "ma�n jjm ae-nn rdddmbrribt => ma�n jm aen rdmbribt, gt: Das de Deum Laudamūs\n",
      "vo--n-nooo  s111-------..t => vonno s1.t          , gt: Anno: 1620.t        \n",
      "pii�teer gnda�  wnndlldeet => pi�ter gnda� wndldet, gt: Pinter gsöll von Nider¬\n",
      "si--nniisoojoo-fffn-nnibr. => sinisojofnnibr.     , gt: Jonas HillePranndt. \n",
      "wo-lddiinng beggern  aerrn => wolding begern aern , gt: besoldūng begern. Inen\n",
      "-------1-----------------3 => 13                  , gt: 189                 \n",
      "Test loss: 140.739242, accuray: 0.000919\n",
      "Character error rate mean: 0.8858; Character error rate sd: 0.4875\n",
      "Word error rate mean: 1.3256; Word error rate sd: 0.6271\n",
      "Total number of images in validation set:     1043\n"
     ]
    }
   ],
   "source": [
    "char_e, w_e = val(crnn, test_dataset, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next things to do:\n",
    "1) Make the word error and character error code robust to empty sets (\"such as give a dummy variable if length < 1\")\n",
    "2) Incorporate character and word error rates into the training set too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_crnn = trained_crnn_dylan\n",
    "if trained_crnn != '':\n",
    "    print('loading pretrained model from %s' % trained_crnn)\n",
    "    crnn.load_state_dict(torch.load(trained_crnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_e\n",
    "char_a = np.array(char_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.77777778  0.5         1.04761905 ...,  1.30769231  0.63636364\n",
      "  0.52631579]\n",
      "0.862\n"
     ]
    }
   ],
   "source": [
    "print(char_a)\n",
    "print(\"%4.3f\" % np.mean(char_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crnn_dylan_trained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d63a2322b89a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#crnn_dylan_trained = crnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcrnn_dylan_trained\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'crnn_dylan_trained' is not defined"
     ]
    }
   ],
   "source": [
    "#crnn_dylan_trained = crnn\n",
    "crnn_dylan_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As of 17 February 2018, the machine is learning to read. It is rough, but it is learning. This is exciting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(niter):\n",
    "    train_iter = iter(train_loader)\n",
    "    i = 0\n",
    "    while i < len(train_loader):\n",
    "        for p in crnn.parameters():\n",
    "            p.requires_grad = True\n",
    "        crnn.train()\n",
    "\n",
    "        cost = trainBatch(crnn, criterion, optimizer)\n",
    "        loss_avg.add(cost)\n",
    "        i += 1\n",
    "\n",
    "        if i % displayInterval == 0:\n",
    "            print('[%d/%d][%d/%d] Loss: %f' %\n",
    "                  (epoch, niter, i, len(train_loader), loss_avg.val()))\n",
    "            loss_avg.reset()\n",
    "\n",
    "        if i % valInterval == 0:\n",
    "            val(crnn, test_dataset, criterion)\n",
    "\n",
    "        # do checkpointing\n",
    "        if i % saveInterval == 0:\n",
    "            torch.save(\n",
    "                crnn.state_dict(), '{0}/netCRNN_{1}_{2}.pth'.format(experiment, epoch, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
